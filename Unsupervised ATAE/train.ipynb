{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "from time import time\n",
    "import import_ipynb\n",
    "import utils as U\n",
    "import codecs\n",
    "\n",
    "logging.basicConfig(\n",
    "    # filename='out.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--ortho-reg'], dest='ortho_reg', nargs=None, const=None, default=0.1, type=<class 'float'>, choices=None, help='The weight of orthogonol regularizaiton (default=0.1)', metavar='<float>')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--out-dir\", dest=\"out_dir_path\", type=str, metavar='<str>', required=True,\n",
    "                    help=\"The path to the output directory\")\n",
    "parser.add_argument(\"-e\", \"--embdim\", dest=\"emb_dim\", type=int, metavar='<int>', default=200,\n",
    "                    help=\"Embeddings dimension (default=200)\")\n",
    "parser.add_argument(\"-b\", \"--batch-size\", dest=\"batch_size\", type=int, metavar='<int>', default=8,\n",
    "                    help=\"Batch size (default=8)\")\n",
    "parser.add_argument(\"-v\", \"--vocab-size\", dest=\"vocab_size\", type=int, metavar='<int>', default=9000,\n",
    "                    help=\"Vocab size. '0' means no limit (default=9000)\")\n",
    "parser.add_argument(\"-as\", \"--aspect-size\", dest=\"aspect_size\", type=int, metavar='<int>', default=14,\n",
    "                    help=\"The number of aspects specified by users (default=14)\")\n",
    "parser.add_argument(\"--emb\", dest=\"emb_path\", type=str, metavar='<str>', help=\"The path to the word embeddings file\")\n",
    "parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, metavar='<int>', default=10,\n",
    "                    help=\"Number of epochs (default=10)\")\n",
    "parser.add_argument(\"-n\", \"--neg-size\", dest=\"neg_size\", type=int, metavar='<int>', default=4,\n",
    "                    help=\"Number of negative instances (default=4)\")\n",
    "parser.add_argument(\"--maxlen\", dest=\"maxlen\", type=int, metavar='<int>', default=0,\n",
    "                    help=\"Maximum allowed number of words during training. '0' means no limit (default=0)\")\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", type=int, metavar='<int>', default=1234, help=\"Random seed (default=1234)\")\n",
    "parser.add_argument(\"-a\", \"--algorithm\", dest=\"algorithm\", type=str, metavar='<str>', default='adam',\n",
    "                    help=\"Optimization algorithm (rmsprop|sgd|adagrad|adadelta|adam|adamax) (default=adam)\")\n",
    "parser.add_argument(\"--ortho-reg\", dest=\"ortho_reg\", type=float, metavar='<float>', default=0.1,\n",
    "                    help=\"The weight of orthogonol regularizaiton (default=0.1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 16:20:21,257 INFO Arguments:\n",
      "2020-10-27 16:20:21,259 INFO   algorithm: adam\n",
      "2020-10-27 16:20:21,259 INFO   aspect_size: 18\n",
      "2020-10-27 16:20:21,260 INFO   batch_size: 512\n",
      "2020-10-27 16:20:21,261 INFO   command: /mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-099f09d8-d224-4058-9041-37eaae76d129.json\n",
      "2020-10-27 16:20:21,262 INFO   emb_dim: 200\n",
      "2020-10-27 16:20:21,263 INFO   emb_path: w2v_embedding\n",
      "2020-10-27 16:20:21,263 INFO   epochs: 50\n",
      "2020-10-27 16:20:21,264 INFO   maxlen: 0\n",
      "2020-10-27 16:20:21,265 INFO   neg_size: 1\n",
      "2020-10-27 16:20:21,266 INFO   ortho_reg: 0.1\n",
      "2020-10-27 16:20:21,267 INFO   out_dir_path: output_dir\n",
      "2020-10-27 16:20:21,268 INFO   seed: 1234\n",
      "2020-10-27 16:20:21,268 INFO   vocab_size: 0\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(\"--emb w2v_embedding --aspect-size 18 -o output_dir --epochs 50 --batch-size 512 --neg-size 1 --algorithm adam --vocab-size 0\".split())\n",
    "out_dir = args.out_dir_path\n",
    "U.mkdir_p(out_dir)\n",
    "U.print_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from reader.ipynb\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import reader as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reader :: Reading data from training set\n",
      " Reader ::  Creating vocab ...\n",
      " Reader ::    909857 total words, 29160 unique words\n",
      " Reader ::  Reading dataset ...\n",
      " Reader ::   train set\n",
      " Reader ::    <num> hit rate: 0.00%, <unk> hit rate: 0.00%\n",
      " Reader ::   test set\n",
      " Reader ::    <num> hit rate: 0.00%, <unk> hit rate: 0.00%\n",
      "175\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "vocab, train_x, test_x, overall_maxlen = dataset.fetch_data(vocab_size=args.vocab_size, maxlen=args.maxlen)\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overall_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overall_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  76111\n",
      "Length of vocab:  29163\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples: ', len(train_x))\n",
    "print('Length of vocab: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_batch_generator(data, batch_size):\n",
    "    n_batch = len(data) // batch_size\n",
    "    batch_count = 0\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    while True:\n",
    "        if batch_count >= n_batch:\n",
    "            np.random.shuffle(data)\n",
    "            batch_count = 0\n",
    "\n",
    "        batch = data[batch_count * batch_size: (batch_count + 1) * batch_size]\n",
    "        batch_count += 1\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_batch_generator(data, batch_size, neg_size):\n",
    "    data_len = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.choice(data_len, batch_size * neg_size)\n",
    "        samples = data[indices].reshape(batch_size, neg_size, dim)\n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from optimizers.ipynb\n"
     ]
    }
   ],
   "source": [
    "from optimizers import get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 16:20:37,732 INFO   Building model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model.ipynb\n",
      "importing Jupyter notebook from custom_layers.ipynb\n",
      "importing Jupyter notebook from w2v_emb_reader.ipynb\n"
     ]
    }
   ],
   "source": [
    "from model import create_model\n",
    "import keras.backend as K\n",
    "\n",
    "logger.info('  Building model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-27 16:20:37,914 INFO Loading embeddings from: w2v_embedding\n",
      "2020-10-27 16:20:37,916 INFO loading Word2Vec object from w2v_embedding\n",
      "2020-10-27 16:20:37,996 INFO loading wv recursively from w2v_embedding.wv.* with mmap=None\n",
      "2020-10-27 16:20:37,997 INFO setting ignored attribute vectors_norm to None\n",
      "2020-10-27 16:20:37,998 INFO loading vocabulary recursively from w2v_embedding.vocabulary.* with mmap=None\n",
      "2020-10-27 16:20:37,999 INFO loading trainables recursively from w2v_embedding.trainables.* with mmap=None\n",
      "2020-10-27 16:20:38,000 INFO setting ignored attribute cum_table to None\n",
      "2020-10-27 16:20:38,001 INFO loaded w2v_embedding\n",
      "2020-10-27 16:20:38,322 INFO   #vectors: 5475, #dimensions: 200\n",
      "2020-10-27 16:20:38,393 INFO Initializing word embedding matrix\n",
      "2020-10-27 16:20:38,523 INFO 5475/29163 word vectors initialized (hit rate: 18.77%)\n",
      "2020-10-27 16:20:38,557 INFO Initializing aspect embedding matrix as centroid of kmean clusters\n"
     ]
    }
   ],
   "source": [
    "model = create_model(args, overall_maxlen, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('word_emb').trainable = False\n",
    "model.compile(optimizer=optimizer, loss=max_margin_loss, metrics=[max_margin_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>  Training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {}\n",
    "\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w\n",
    "\n",
    "sen_gen = sent_batch_generator(train_x, args.batch_size)\n",
    "neg_gen = neg_batch_generator(train_x, args.batch_size, args.neg_size)\n",
    "batches_per_epoch = len(train_x) // args.batch_size\n",
    "#batches_per_epoch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch 148\n"
     ]
    }
   ],
   "source": [
    "print(\"Batches per epoch\", batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:34<00:00,  4.29it/s]\n",
      "2020-10-27 16:21:20,103 INFO Epoch 0, train: 34s\n",
      "2020-10-27 16:21:20,104 INFO Total loss: 0.8283, max_margin_loss: 0.7407, ortho_reg: 0.0876\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.45it/s]\n",
      "2020-10-27 16:21:53,615 INFO Epoch 1, train: 33s\n",
      "2020-10-27 16:21:53,616 INFO Total loss: 0.5703, max_margin_loss: 0.5684, ortho_reg: 0.0019\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.55it/s]\n",
      "2020-10-27 16:22:26,393 INFO Epoch 2, train: 32s\n",
      "2020-10-27 16:22:26,394 INFO Total loss: 0.5031, max_margin_loss: 0.5014, ortho_reg: 0.0017\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.51it/s]\n",
      "2020-10-27 16:22:59,504 INFO Epoch 3, train: 32s\n",
      "2020-10-27 16:22:59,505 INFO Total loss: 0.4869, max_margin_loss: 0.4853, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.62it/s]\n",
      "2020-10-27 16:23:31,792 INFO Epoch 4, train: 32s\n",
      "2020-10-27 16:23:31,794 INFO Total loss: 0.4791, max_margin_loss: 0.4775, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\n",
      "2020-10-27 16:24:04,180 INFO Epoch 5, train: 32s\n",
      "2020-10-27 16:24:04,181 INFO Total loss: 0.4749, max_margin_loss: 0.4733, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\n",
      "2020-10-27 16:24:36,602 INFO Epoch 6, train: 32s\n",
      "2020-10-27 16:24:36,603 INFO Total loss: 0.4722, max_margin_loss: 0.4706, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.61it/s]\n",
      "2020-10-27 16:25:08,974 INFO Epoch 7, train: 32s\n",
      "2020-10-27 16:25:08,975 INFO Total loss: 0.4699, max_margin_loss: 0.4683, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\n",
      "2020-10-27 16:25:41,400 INFO Epoch 8, train: 32s\n",
      "2020-10-27 16:25:41,401 INFO Total loss: 0.4672, max_margin_loss: 0.4656, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.50it/s]\n",
      "2020-10-27 16:26:14,574 INFO Epoch 9, train: 32s\n",
      "2020-10-27 16:26:14,576 INFO Total loss: 0.4670, max_margin_loss: 0.4655, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.46it/s]\n",
      "2020-10-27 16:26:47,988 INFO Epoch 10, train: 33s\n",
      "2020-10-27 16:26:47,990 INFO Total loss: 0.4654, max_margin_loss: 0.4639, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.49it/s]\n",
      "2020-10-27 16:27:21,244 INFO Epoch 11, train: 32s\n",
      "2020-10-27 16:27:21,246 INFO Total loss: 0.4648, max_margin_loss: 0.4632, ortho_reg: 0.0016\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2020-10-27 16:27:55,185 INFO Epoch 12, train: 33s\n",
      "2020-10-27 16:27:55,185 INFO Total loss: 0.4649, max_margin_loss: 0.4633, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.38it/s]\n",
      "2020-10-27 16:28:29,185 INFO Epoch 13, train: 33s\n",
      "2020-10-27 16:28:29,190 INFO Total loss: 0.4637, max_margin_loss: 0.4622, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.45it/s]\n",
      "2020-10-27 16:29:02,714 INFO Epoch 14, train: 33s\n",
      "2020-10-27 16:29:02,716 INFO Total loss: 0.4636, max_margin_loss: 0.4620, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.45it/s]\n",
      "2020-10-27 16:29:36,226 INFO Epoch 15, train: 33s\n",
      "2020-10-27 16:29:36,227 INFO Total loss: 0.4634, max_margin_loss: 0.4619, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.40it/s]\n",
      "2020-10-27 16:30:10,136 INFO Epoch 16, train: 33s\n",
      "2020-10-27 16:30:10,138 INFO Total loss: 0.4616, max_margin_loss: 0.4601, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.38it/s]\n",
      "2020-10-27 16:30:43,915 INFO Epoch 17, train: 33s\n",
      "2020-10-27 16:30:43,916 INFO Total loss: 0.4620, max_margin_loss: 0.4604, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:34<00:00,  4.29it/s]\n",
      "2020-10-27 16:31:18,446 INFO Epoch 18, train: 34s\n",
      "2020-10-27 16:31:18,447 INFO Total loss: 0.4616, max_margin_loss: 0.4601, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:35<00:00,  4.20it/s]\n",
      "2020-10-27 16:31:53,677 INFO Epoch 19, train: 35s\n",
      "2020-10-27 16:31:53,678 INFO Total loss: 0.4618, max_margin_loss: 0.4603, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:34<00:00,  4.27it/s]\n",
      "2020-10-27 16:32:28,351 INFO Epoch 20, train: 34s\n",
      "2020-10-27 16:32:28,352 INFO Total loss: 0.4618, max_margin_loss: 0.4603, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:34<00:00,  4.27it/s]\n",
      "2020-10-27 16:33:03,266 INFO Epoch 21, train: 34s\n",
      "2020-10-27 16:33:03,268 INFO Total loss: 0.4613, max_margin_loss: 0.4598, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.41it/s]\n",
      "2020-10-27 16:33:36,817 INFO Epoch 22, train: 33s\n",
      "2020-10-27 16:33:36,818 INFO Total loss: 0.4614, max_margin_loss: 0.4598, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.47it/s]\n",
      "2020-10-27 16:34:10,154 INFO Epoch 23, train: 33s\n",
      "2020-10-27 16:34:10,155 INFO Total loss: 0.4610, max_margin_loss: 0.4595, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.47it/s]\n",
      "2020-10-27 16:34:43,510 INFO Epoch 24, train: 33s\n",
      "2020-10-27 16:34:43,511 INFO Total loss: 0.4608, max_margin_loss: 0.4593, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.51it/s]\n",
      "2020-10-27 16:35:16,608 INFO Epoch 25, train: 32s\n",
      "2020-10-27 16:35:16,610 INFO Total loss: 0.4600, max_margin_loss: 0.4585, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.46it/s]\n",
      "2020-10-27 16:35:49,819 INFO Epoch 26, train: 33s\n",
      "2020-10-27 16:35:49,820 INFO Total loss: 0.4603, max_margin_loss: 0.4588, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.40it/s]\n",
      "2020-10-27 16:36:23,726 INFO Epoch 27, train: 33s\n",
      "2020-10-27 16:36:23,728 INFO Total loss: 0.4597, max_margin_loss: 0.4582, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.37it/s]\n",
      "2020-10-27 16:36:57,626 INFO Epoch 28, train: 33s\n",
      "2020-10-27 16:36:57,627 INFO Total loss: 0.4599, max_margin_loss: 0.4584, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.50it/s]\n",
      "2020-10-27 16:37:30,544 INFO Epoch 29, train: 32s\n",
      "2020-10-27 16:37:30,544 INFO Total loss: 0.4602, max_margin_loss: 0.4587, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.55it/s]\n",
      "2020-10-27 16:38:03,044 INFO Epoch 30, train: 32s\n",
      "2020-10-27 16:38:03,045 INFO Total loss: 0.4602, max_margin_loss: 0.4587, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.56it/s]\n",
      "2020-10-27 16:38:35,726 INFO Epoch 31, train: 32s\n",
      "2020-10-27 16:38:35,728 INFO Total loss: 0.4592, max_margin_loss: 0.4578, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\n",
      "2020-10-27 16:39:07,700 INFO Epoch 32, train: 31s\n",
      "2020-10-27 16:39:07,701 INFO Total loss: 0.4597, max_margin_loss: 0.4582, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\n",
      "2020-10-27 16:39:39,694 INFO Epoch 33, train: 31s\n",
      "2020-10-27 16:39:39,695 INFO Total loss: 0.4599, max_margin_loss: 0.4585, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.61it/s]\n",
      "2020-10-27 16:40:11,811 INFO Epoch 34, train: 32s\n",
      "2020-10-27 16:40:11,812 INFO Total loss: 0.4595, max_margin_loss: 0.4581, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\n",
      "2020-10-27 16:40:44,246 INFO Epoch 35, train: 32s\n",
      "2020-10-27 16:40:44,248 INFO Total loss: 0.4585, max_margin_loss: 0.4570, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.52it/s]\n",
      "2020-10-27 16:41:17,005 INFO Epoch 36, train: 32s\n",
      "2020-10-27 16:41:17,006 INFO Total loss: 0.4585, max_margin_loss: 0.4570, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.48it/s]\n",
      "2020-10-27 16:41:50,044 INFO Epoch 37, train: 33s\n",
      "2020-10-27 16:41:50,045 INFO Total loss: 0.4586, max_margin_loss: 0.4571, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.51it/s]\n",
      "2020-10-27 16:42:23,112 INFO Epoch 38, train: 32s\n",
      "2020-10-27 16:42:23,114 INFO Total loss: 0.4582, max_margin_loss: 0.4568, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.58it/s]\n",
      "2020-10-27 16:42:55,469 INFO Epoch 39, train: 32s\n",
      "2020-10-27 16:42:55,470 INFO Total loss: 0.4592, max_margin_loss: 0.4578, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\n",
      "2020-10-27 16:43:27,444 INFO Epoch 40, train: 31s\n",
      "2020-10-27 16:43:27,446 INFO Total loss: 0.4586, max_margin_loss: 0.4572, ortho_reg: 0.0015\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.58it/s]\n",
      "2020-10-27 16:43:59,745 INFO Epoch 41, train: 32s\n",
      "2020-10-27 16:43:59,746 INFO Total loss: 0.4585, max_margin_loss: 0.4570, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\n",
      "2020-10-27 16:44:31,972 INFO Epoch 42, train: 31s\n",
      "2020-10-27 16:44:31,974 INFO Total loss: 0.4581, max_margin_loss: 0.4567, ortho_reg: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\n",
      "2020-10-27 16:45:04,176 INFO Epoch 43, train: 32s\n",
      "2020-10-27 16:45:04,177 INFO Total loss: 0.4583, max_margin_loss: 0.4569, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.59it/s]\n",
      "2020-10-27 16:45:36,429 INFO Epoch 44, train: 32s\n",
      "2020-10-27 16:45:36,430 INFO Total loss: 0.4582, max_margin_loss: 0.4568, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.49it/s]\n",
      "2020-10-27 16:46:09,616 INFO Epoch 45, train: 32s\n",
      "2020-10-27 16:46:09,617 INFO Total loss: 0.4579, max_margin_loss: 0.4564, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.47it/s]\n",
      "2020-10-27 16:46:42,995 INFO Epoch 46, train: 33s\n",
      "2020-10-27 16:46:42,997 INFO Total loss: 0.4579, max_margin_loss: 0.4564, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:33<00:00,  4.48it/s]\n",
      "2020-10-27 16:47:16,262 INFO Epoch 47, train: 33s\n",
      "2020-10-27 16:47:16,263 INFO Total loss: 0.4576, max_margin_loss: 0.4562, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.50it/s]\n",
      "2020-10-27 16:47:49,437 INFO Epoch 48, train: 32s\n",
      "2020-10-27 16:47:49,438 INFO Total loss: 0.4572, max_margin_loss: 0.4558, ortho_reg: 0.0014\n",
      "100%|██████████| 148/148 [00:32<00:00,  4.58it/s]\n",
      "2020-10-27 16:48:21,776 INFO Epoch 49, train: 32s\n",
      "2020-10-27 16:48:21,777 INFO Total loss: 0.4583, max_margin_loss: 0.4570, ortho_reg: 0.0014\n"
     ]
    }
   ],
   "source": [
    "batch_var = 0\n",
    "batch_no = []\n",
    "Total_loss = []\n",
    "Total_max_margin_loss = []\n",
    "final_aspect_list = []\n",
    "for ii in range(args.epochs):\n",
    "    t0 = time()\n",
    "    loss, max_margin_loss = 0., 0.\n",
    "\n",
    "    for b in tqdm(range(batches_per_epoch)):\n",
    "        sen_input = next(sen_gen)\n",
    "        neg_input = next(neg_gen)\n",
    "\n",
    "        try:\n",
    "            batch_loss, batch_max_margin_loss = model.train_on_batch([sen_input,neg_input], np.ones((args.batch_size, 1)))\n",
    "            #print(model.test_on_batch([sen_input,neg_input], np.ones((args.batch_size, 1))))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(sen_input.shape, sen_input)\n",
    "            print(neg_input.shape, neg_input)\n",
    "\n",
    "            print()\n",
    "            quit()\n",
    "\n",
    "        loss += batch_loss / batches_per_epoch\n",
    "        max_margin_loss += batch_max_margin_loss / batches_per_epoch\n",
    "\n",
    "    tr_time = time() - t0\n",
    "\n",
    "    if loss < min_loss:\n",
    "\n",
    "        min_loss = loss\n",
    "        word_emb = K.get_value(model.get_layer('word_emb').embeddings)\n",
    "        aspect_emb = K.get_value(model.get_layer('aspect_emb').W)\n",
    "        word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "        aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "        aspect_file = codecs.open(out_dir + '/aspect_adamlr1.log', 'w', 'utf-8')\n",
    "        aspect_txt = codecs.open(out_dir + '/aspect_adamlr1.txt', 'w', 'utf-8')\n",
    "        model.save_weights(out_dir + '/model_param_adamlr1')\n",
    "        \n",
    "        for ind in range(len(aspect_emb)):\n",
    "            aspect_txt.write('[')\n",
    "            desc = aspect_emb[ind]\n",
    "            sims = word_emb.dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "            desc_list = [vocab_inv[w] + \":\" + str(sims[w]) for w in ordered_words[:30]]\n",
    "            asp_list = [\"'\" + vocab_inv[w] +\"',\" for w in ordered_words[:30]]\n",
    "            #final_aspect_list = asp_list\n",
    "            aspect_txt.write(''.join(asp_list))\n",
    "            #print('Aspect %d:' % ind)\n",
    "            #print(desc_list)\n",
    "            #print(asp_list)\n",
    "            aspect_file.write('Aspect %d:\\n' % ind)\n",
    "            aspect_file.write(' '.join(desc_list) + '\\n\\n')\n",
    "            aspect_txt.write('],')\n",
    "        \n",
    "    logger.info('Epoch %d, train: %is' % (ii, tr_time))\n",
    "    logger.info(\n",
    "        'Total loss: %.4f, max_margin_loss: %.4f, ortho_reg: %.4f' % (loss, max_margin_loss, loss - max_margin_loss))\n",
    "    #logger.info(model.test_on_batch([sen_input,neg_input], np.ones((args.batch_size, 1))))\n",
    "    batch_var = batch_var + 1\n",
    "    batch_no.append(batch_var)\n",
    "    Total_loss.append(loss)\n",
    "    Total_max_margin_loss.append(max_margin_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_input (InputLayer)     (None, 175)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            multiple             5832600     sentence_input[0][0]             \n",
      "                                                                 neg_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average__layer_1 (Average_Layer (None, 200)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "att_weights (Attention_Layer)   (None, 175)          40001       word_emb[0][0]                   \n",
      "                                                                 average__layer_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weighted_sum__layer_1 (Weighted (None, 200)          0           word_emb[0][0]                   \n",
      "                                                                 att_weights[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "neg_input (InputLayer)          (None, 1, 175)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 18)           3618        weighted_sum__layer_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_t (Activation)                (None, 18)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average__layer_2 (Average_Layer (None, 1, 200)       0           word_emb[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aspect_emb (WeightedAspectEmb_L (None, 200)          3600        p_t[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_margin (MaxMargin_Layer)    (None, 1)            0           weighted_sum__layer_1[0][0]      \n",
      "                                                                 average__layer_2[0][0]           \n",
      "                                                                 aspect_emb[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,879,819\n",
      "Trainable params: 47,219\n",
      "Non-trainable params: 5,832,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"epochs = \")\n",
    "# print(batch_no)\n",
    "# print (\"\\n\")\n",
    "# print(\"Total Loss = \")\n",
    "# print(Total_loss)\n",
    "# print (\"\\n\")\n",
    "# print(\"Max Margin Loss = \" )\n",
    "# print(Total_max_margin_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Total_max_margin_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"output_dir/train_model_adamlr1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
