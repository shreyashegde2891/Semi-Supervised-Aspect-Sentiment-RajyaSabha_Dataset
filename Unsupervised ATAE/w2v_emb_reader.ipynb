{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbReader:\n",
    "\n",
    "    def __init__(self, emb_Inputpath, emb_dim=None):\n",
    "\n",
    "        logger.info('Loading embeddings from: ' + emb_Inputpath)\n",
    "        self.embeddings = {}\n",
    "        emb_matrix = []\n",
    "\n",
    "        model = gensim.models.Word2Vec.load(emb_Inputpath)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        for word in model.wv.vocab:\n",
    "            self.embeddings[word] = list(model.wv[word])\n",
    "            emb_matrix.append(list(model.wv[word]))\n",
    "\n",
    "        if emb_dim is not None:\n",
    "            assert self.emb_dim == len(self.embeddings[word])\n",
    "\n",
    "        self.vector_size = len(self.embeddings)\n",
    "        self.emb_matrix = np.asarray(emb_matrix)\n",
    "\n",
    "        logger.info('  #vectors: %i, #dimensions: %i' % (self.vector_size, self.emb_dim))\n",
    "\n",
    "    def get_emb_from_word(self, word):\n",
    "\n",
    "        try:\n",
    "            return self.embeddings[word]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def get_emb_matrix_from_vocab(self, vocab, emb_matrix):\n",
    "\n",
    "        counter = 0.\n",
    "        for word, index in vocab.items():\n",
    "            try:\n",
    "                emb_matrix[index] = self.embeddings[word]\n",
    "                counter += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        logger.info(\n",
    "            '%i/%i word vectors initialized (hit rate: %.2f%%)' % (counter, len(vocab), 100 * counter / len(vocab)))\n",
    "        # L2 normalization\n",
    "        norm_emb_matrix = emb_matrix / np.linalg.norm(emb_matrix, axis=-1, keepdims=True)\n",
    "\n",
    "        return norm_emb_matrix\n",
    "\n",
    "    def get_aspect_matrix(self, n_clusters):\n",
    "\n",
    "        km = KMeans(n_clusters=n_clusters)\n",
    "        km.fit(self.emb_matrix)\n",
    "        clusters = km.cluster_centers_\n",
    "\n",
    " \n",
    "        norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "        return norm_aspect_matrix.astype(np.float32)\n",
    "\n",
    "    def return_emb_dim(self):\n",
    "        return self.emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
