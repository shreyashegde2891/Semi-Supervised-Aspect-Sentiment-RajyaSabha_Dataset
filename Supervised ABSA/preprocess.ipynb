{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from config.ipynb\n",
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import import_ipynb\n",
    "from config import Config\n",
    "from utils import pickle_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input_data/input.csv')\n",
    "train = df.sample(frac=0.7)\n",
    "test = df.loc[~df.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>question</th>\n",
       "      <th>aspect</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>whether government is aware that there is spur...</td>\n",
       "      <td>last</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>the details of central universities which have...</td>\n",
       "      <td>universities</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>the total number of central universities which...</td>\n",
       "      <td>universities</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>whether government is aware that most of the p...</td>\n",
       "      <td>drinking</td>\n",
       "      <td>54</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that the method adopted b...</td>\n",
       "      <td>banks</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>whether any proposal of merger of anyonya bank...</td>\n",
       "      <td>bank</td>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that physiotherapists hav...</td>\n",
       "      <td>nursing</td>\n",
       "      <td>101</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>the details of the commission structure of int...</td>\n",
       "      <td>last</td>\n",
       "      <td>170</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>the details of commitment charges paid to diff...</td>\n",
       "      <td>last</td>\n",
       "      <td>173</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>whether it is a fact that the state government...</td>\n",
       "      <td>credit</td>\n",
       "      <td>130</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>whether the world bank has recently decided to...</td>\n",
       "      <td>bank</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>whether the operations of nationalized banks l...</td>\n",
       "      <td>nationalized</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>the total amount of loan disbursed by the bank...</td>\n",
       "      <td>bank</td>\n",
       "      <td>127</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>whether government is aware of anomalies after...</td>\n",
       "      <td>anomalies</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>whether it is a fact that that the pm`s honora...</td>\n",
       "      <td>banks</td>\n",
       "      <td>213</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>whether government s considering far reaching ...</td>\n",
       "      <td>banks</td>\n",
       "      <td>89</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>whether the logos of many public sector banks ...</td>\n",
       "      <td>banks</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>whether government s attention has been drawn ...</td>\n",
       "      <td>court</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>whether it is a fact that government`s leading...</td>\n",
       "      <td>bank</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>the details of the political parties and their...</td>\n",
       "      <td>last</td>\n",
       "      <td>103</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>whether it is fact that state bank of india sb...</td>\n",
       "      <td>bank</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>whether government has decided to cut down the...</td>\n",
       "      <td>court</td>\n",
       "      <td>63</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>whether the country has nearly 2000 archaic la...</td>\n",
       "      <td>last</td>\n",
       "      <td>134</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>whether fluoride contaminated drinking water a...</td>\n",
       "      <td>drinking</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>the total number of watershed projects sanctio...</td>\n",
       "      <td>last</td>\n",
       "      <td>175</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>whether a committee appointed by government ha...</td>\n",
       "      <td>calls</td>\n",
       "      <td>141</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>the targets set for providing shelters under t...</td>\n",
       "      <td>last</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that state government of ...</td>\n",
       "      <td>road</td>\n",
       "      <td>140</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2</td>\n",
       "      <td>the details of loan provided to people in the ...</td>\n",
       "      <td>last</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that the central governme...</td>\n",
       "      <td>cghs</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31970</th>\n",
       "      <td>2</td>\n",
       "      <td>whether it is a fact that the indian handloom ...</td>\n",
       "      <td>handloom</td>\n",
       "      <td>37</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31972</th>\n",
       "      <td>0</td>\n",
       "      <td>whether post harvest storage of agricultural p...</td>\n",
       "      <td>produces</td>\n",
       "      <td>45</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>1</td>\n",
       "      <td>the quantum of decline in area of sugarcane cu...</td>\n",
       "      <td>last</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31975</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that india has developed ...</td>\n",
       "      <td>last</td>\n",
       "      <td>91</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31977</th>\n",
       "      <td>2</td>\n",
       "      <td>whether government has details of the number o...</td>\n",
       "      <td>produces</td>\n",
       "      <td>104</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31978</th>\n",
       "      <td>1</td>\n",
       "      <td>the details of the farmers who have left farmi...</td>\n",
       "      <td>last</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31979</th>\n",
       "      <td>1</td>\n",
       "      <td>whether the states of andhra pradesh and telan...</td>\n",
       "      <td>last</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>2</td>\n",
       "      <td>whether the pradhan mantri fasal bima yojana p...</td>\n",
       "      <td>yojana</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>1</td>\n",
       "      <td>whether government has put in place a roadmap ...</td>\n",
       "      <td>water</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31982</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that maharashtra is one o...</td>\n",
       "      <td>produces</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31983</th>\n",
       "      <td>1</td>\n",
       "      <td>the details of funds allocated to various stat...</td>\n",
       "      <td>last</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31985</th>\n",
       "      <td>1</td>\n",
       "      <td>whether farmers are shifting from growing oils...</td>\n",
       "      <td>oilseeds</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31987</th>\n",
       "      <td>0</td>\n",
       "      <td>whether government has received any communicat...</td>\n",
       "      <td>bank</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31988</th>\n",
       "      <td>1</td>\n",
       "      <td>whether it is a fact that government has been ...</td>\n",
       "      <td>cable</td>\n",
       "      <td>66</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31990</th>\n",
       "      <td>0</td>\n",
       "      <td>whether it is a fact that the telecom companie...</td>\n",
       "      <td>last</td>\n",
       "      <td>97</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>1</td>\n",
       "      <td>whether competition occurring in communication...</td>\n",
       "      <td>last</td>\n",
       "      <td>127</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>1</td>\n",
       "      <td>the details of average annual estimated demand...</td>\n",
       "      <td>last</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>0</td>\n",
       "      <td>the details of funds allocated for modernisati...</td>\n",
       "      <td>last</td>\n",
       "      <td>123</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>1</td>\n",
       "      <td>whether government has any data of officers wo...</td>\n",
       "      <td>railway</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32009</th>\n",
       "      <td>0</td>\n",
       "      <td>the steps railways are taking to stop the inci...</td>\n",
       "      <td>railways</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32010</th>\n",
       "      <td>0</td>\n",
       "      <td>whether adverse remarks have been made in the ...</td>\n",
       "      <td>railways</td>\n",
       "      <td>121</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32012</th>\n",
       "      <td>2</td>\n",
       "      <td>the steps taken for the safety of passengers d...</td>\n",
       "      <td>railways</td>\n",
       "      <td>68</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32013</th>\n",
       "      <td>2</td>\n",
       "      <td>whether installation of high definition cctv c...</td>\n",
       "      <td>coaches</td>\n",
       "      <td>206</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32015</th>\n",
       "      <td>1</td>\n",
       "      <td>whether work relating to doubling and electrif...</td>\n",
       "      <td>tracks</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32019</th>\n",
       "      <td>1</td>\n",
       "      <td>the grounds on which concession is provided on...</td>\n",
       "      <td>train</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32020</th>\n",
       "      <td>1</td>\n",
       "      <td>the total number of different types of goods w...</td>\n",
       "      <td>railways</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32023</th>\n",
       "      <td>1</td>\n",
       "      <td>whether railways propose to install a large nu...</td>\n",
       "      <td>railways</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32027</th>\n",
       "      <td>0</td>\n",
       "      <td>the number of persons killed while travelling ...</td>\n",
       "      <td>last</td>\n",
       "      <td>101</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32030</th>\n",
       "      <td>1</td>\n",
       "      <td>whether for a new line from puri to konark gov...</td>\n",
       "      <td>railways</td>\n",
       "      <td>155</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32031</th>\n",
       "      <td>2</td>\n",
       "      <td>the total revenue earned by railways from non ...</td>\n",
       "      <td>railways</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9612 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                           question  \\\n",
       "0              1  whether government is aware that there is spur...   \n",
       "5              1  the details of central universities which have...   \n",
       "8              1  the total number of central universities which...   \n",
       "16             1  whether government is aware that most of the p...   \n",
       "21             1  whether it is a fact that the method adopted b...   \n",
       "23             1  whether any proposal of merger of anyonya bank...   \n",
       "24             1  whether it is a fact that physiotherapists hav...   \n",
       "26             1  the details of the commission structure of int...   \n",
       "27             2  the details of commitment charges paid to diff...   \n",
       "29             2  whether it is a fact that the state government...   \n",
       "33             1  whether the world bank has recently decided to...   \n",
       "36             1  whether the operations of nationalized banks l...   \n",
       "37             1  the total amount of loan disbursed by the bank...   \n",
       "42             1  whether government is aware of anomalies after...   \n",
       "44             2  whether it is a fact that that the pm`s honora...   \n",
       "45             2  whether government s considering far reaching ...   \n",
       "49             1  whether the logos of many public sector banks ...   \n",
       "52             2  whether government s attention has been drawn ...   \n",
       "54             2  whether it is a fact that government`s leading...   \n",
       "55             2  the details of the political parties and their...   \n",
       "57             0  whether it is fact that state bank of india sb...   \n",
       "69             2  whether government has decided to cut down the...   \n",
       "78             1  whether the country has nearly 2000 archaic la...   \n",
       "83             1  whether fluoride contaminated drinking water a...   \n",
       "84             1  the total number of watershed projects sanctio...   \n",
       "85             0  whether a committee appointed by government ha...   \n",
       "87             1  the targets set for providing shelters under t...   \n",
       "88             1  whether it is a fact that state government of ...   \n",
       "89             2  the details of loan provided to people in the ...   \n",
       "95             1  whether it is a fact that the central governme...   \n",
       "...          ...                                                ...   \n",
       "31970          2  whether it is a fact that the indian handloom ...   \n",
       "31972          0  whether post harvest storage of agricultural p...   \n",
       "31974          1  the quantum of decline in area of sugarcane cu...   \n",
       "31975          1  whether it is a fact that india has developed ...   \n",
       "31977          2  whether government has details of the number o...   \n",
       "31978          1  the details of the farmers who have left farmi...   \n",
       "31979          1  whether the states of andhra pradesh and telan...   \n",
       "31980          2  whether the pradhan mantri fasal bima yojana p...   \n",
       "31981          1  whether government has put in place a roadmap ...   \n",
       "31982          1  whether it is a fact that maharashtra is one o...   \n",
       "31983          1  the details of funds allocated to various stat...   \n",
       "31985          1  whether farmers are shifting from growing oils...   \n",
       "31987          0  whether government has received any communicat...   \n",
       "31988          1  whether it is a fact that government has been ...   \n",
       "31990          0  whether it is a fact that the telecom companie...   \n",
       "31993          1  whether competition occurring in communication...   \n",
       "31994          1  the details of average annual estimated demand...   \n",
       "31998          0  the details of funds allocated for modernisati...   \n",
       "32000          1  whether government has any data of officers wo...   \n",
       "32009          0  the steps railways are taking to stop the inci...   \n",
       "32010          0  whether adverse remarks have been made in the ...   \n",
       "32012          2  the steps taken for the safety of passengers d...   \n",
       "32013          2  whether installation of high definition cctv c...   \n",
       "32015          1  whether work relating to doubling and electrif...   \n",
       "32019          1  the grounds on which concession is provided on...   \n",
       "32020          1  the total number of different types of goods w...   \n",
       "32023          1  whether railways propose to install a large nu...   \n",
       "32027          0  the number of persons killed while travelling ...   \n",
       "32030          1  whether for a new line from puri to konark gov...   \n",
       "32031          2  the total revenue earned by railways from non ...   \n",
       "\n",
       "             aspect  from   to  \n",
       "0              last    88   92  \n",
       "5      universities    23   35  \n",
       "8      universities    28   40  \n",
       "16         drinking    54   62  \n",
       "21            banks    48   53  \n",
       "23             bank    42   46  \n",
       "24          nursing   101  108  \n",
       "26             last   170  174  \n",
       "27             last   173  177  \n",
       "29           credit   130  136  \n",
       "33             bank    18   22  \n",
       "36     nationalized    26   38  \n",
       "37             bank   127  131  \n",
       "42        anomalies    31   40  \n",
       "44            banks   213  218  \n",
       "45            banks    89   94  \n",
       "49            banks    40   45  \n",
       "52            court    85   90  \n",
       "54             bank    47   51  \n",
       "55             last   103  107  \n",
       "57             bank    30   34  \n",
       "69            court    63   68  \n",
       "78             last   134  138  \n",
       "83         drinking    30   38  \n",
       "84             last   175  179  \n",
       "85            calls   141  146  \n",
       "87             last    78   82  \n",
       "88             road   140  144  \n",
       "89             last   108  112  \n",
       "95             cghs    63   67  \n",
       "...             ...   ...  ...  \n",
       "31970      handloom    37   45  \n",
       "31972      produces    45   53  \n",
       "31974          last    67   71  \n",
       "31975          last    91   95  \n",
       "31977      produces   104  112  \n",
       "31978          last    60   64  \n",
       "31979          last    88   92  \n",
       "31980        yojana    38   44  \n",
       "31981         water    76   81  \n",
       "31982      produces    80   88  \n",
       "31983          last    86   90  \n",
       "31985      oilseeds    42   50  \n",
       "31987          bank    65   69  \n",
       "31988         cable    66   71  \n",
       "31990          last    97  101  \n",
       "31993          last   127  131  \n",
       "31994          last    83   87  \n",
       "31998          last   123  127  \n",
       "32000       railway    63   70  \n",
       "32009      railways    10   18  \n",
       "32010      railways   121  129  \n",
       "32012      railways    68   76  \n",
       "32013       coaches   206  213  \n",
       "32015        tracks    57   63  \n",
       "32019         train    47   52  \n",
       "32020      railways    71   79  \n",
       "32023      railways     8   16  \n",
       "32027          last   101  105  \n",
       "32030      railways   155  163  \n",
       "32031      railways    28   36  \n",
       "\n",
       "[9612 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(train.columns[0],axis=1,inplace=True)\n",
    "test.drop(test.columns[0],axis=1,inplace=True)\n",
    "train.to_csv('data/train.csv',index=False)\n",
    "test.to_csv('data/test.csv',index=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    word_vec = {}\n",
    "    emd_dim = -1\n",
    "    with open(filename, 'r', encoding = \"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            word_vector = np.array([float(v) for v in line[1:]])\n",
    "            word_vec[word] = word_vector\n",
    "            if emd_dim == -1:\n",
    "                emd_dim = len(word_vector)\n",
    "\n",
    "    assert (len(vw) == emd_dim for vw in word_vec.values())\n",
    "\n",
    "    return word_vec, emd_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(l):\n",
    "    result = list()\n",
    "    for item in l:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            result.extend(item)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus, start_id=1):\n",
    "    corpus = flat_list(corpus)\n",
    "    return dict((word, idx) for idx, word in enumerate(set(corpus), start=start_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding(corpus, vocab, embedding_dim=300):\n",
    "    model = Word2Vec(corpus, size=embedding_dim, min_count=1, window=5, sg=1, iter=10)\n",
    "    weights = model.wv.syn0\n",
    "    d = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    emb = np.zeros(shape=(len(vocab) + 2, embedding_dim), dtype='float32')\n",
    "    count = 0\n",
    "    for w, i in vocab.items():\n",
    "        if w not in d:\n",
    "            count += 1\n",
    "            emb[i, :] = np.random.uniform(-0.1, 0.1, embedding_dim)\n",
    "        else:\n",
    "            emb[i, :] = weights[d[w], :]\n",
    "    print('embedding out of vocabulary：', count)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove_embedding(vocab, word_vectors, embed_dim):\n",
    "    emb_matrix = np.zeros(shape=(len(vocab) + 2, embed_dim), dtype='float32')\n",
    "\n",
    "    count = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word not in word_vectors:\n",
    "            count += 1\n",
    "            emb_matrix[i, :] = np.random.uniform(-0.1, 0.1, embed_dim)\n",
    "        else:\n",
    "            emb_matrix[i, :] = word_vectors[word]\n",
    "    print('glove embedding out of vocabulary：', count)\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_asp_embedding(aspect_vocab, split_func, word_vocab, word_embed):\n",
    "    asp_emb = np.random.uniform(-0.1, 0.1, [len(aspect_vocab.keys()), word_embed.shape[1]])\n",
    "    count = 0\n",
    "    for aspect, aspect_id in aspect_vocab.items():\n",
    "        word_ids = [word_vocab.get(word, 0) for word in split_func(aspect)]\n",
    "        if any(word_ids):\n",
    "            avg_vector = np.mean(word_embed[word_ids], axis=0)\n",
    "            asp_emb[aspect_id] = avg_vector\n",
    "        else:\n",
    "            count += 1\n",
    "    print('aspect embedding out of vocabulary:', count)\n",
    "    return asp_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_asp_text_embedding(aspect_text_vocab, word_vocab, word_embed):\n",
    "    asp_text_embed = np.zeros(shape=(len(aspect_text_vocab) + 2, word_embed.shape[1]), dtype='float32')\n",
    "    count = 0\n",
    "    for aspect, aspect_id in aspect_text_vocab.items():\n",
    "        if aspect in word_vocab:\n",
    "            asp_text_embed[aspect_id] = word_embed[word_vocab[aspect]]\n",
    "        else:\n",
    "            count += 1\n",
    "            asp_text_embed[aspect_id] = np.random.uniform(-0.1, 0.1, word_embed.shape[1])\n",
    "    print('aspect text embedding out of vocabulary:', count)\n",
    "    return asp_text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_len_distribution(train_input, test_input):\n",
    "    text_len = list()\n",
    "    text_len.extend([len(l) for l in train_input])\n",
    "    text_len.extend([len(l) for l in test_input])\n",
    "    max_len = np.max(text_len)\n",
    "    min_len = np.min(text_len)\n",
    "    avg_len = np.average(text_len)\n",
    "    median_len = np.median(text_len)\n",
    "    print('max len:', max_len, 'min_len', min_len, 'avg len', avg_len, 'median len', median_len)\n",
    "    for i in range(int(median_len), int(max_len), 5):\n",
    "        less = list(filter(lambda x: x <= i, text_len))\n",
    "        ratio = len(less) / len(text_len)\n",
    "        print(i, ratio)\n",
    "        if ratio >= 0.99:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_distribution(labels):\n",
    "    for cls, count in Counter(labels).most_common():\n",
    "        print(cls, count, count / len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_info(l, start, end):\n",
    "    pos_info = []\n",
    "    offset_info =[]\n",
    "    for i in range(len(l)):\n",
    "        if i < start:\n",
    "            pos_info.append(1 - abs(i - start) / len(l))\n",
    "            offset_info.append(i - start)\n",
    "        elif start <= i < end:\n",
    "            pos_info.append(1.)\n",
    "            offset_info.append(0.)\n",
    "        else:\n",
    "            pos_info.append(1 - abs(i - end + 1) / len(l))\n",
    "            offset_info.append(i - end +1)\n",
    "    return pos_info, offset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_get_loc_info(data, word_vocab, char_vocab, word_cut_func):\n",
    "    word_input_l, word_input_r, word_input_r_with_pad, word_pos_input, word_offset_input = [], [], [], [], []\n",
    "    char_input_l, char_input_r, char_input_r_with_pad, char_pos_input, char_offset_input = [], [], [], [], []\n",
    "    word_mask, char_mask = [], []\n",
    "    for idx, row in data.iterrows():\n",
    "        text, word_list, char_list, aspect = row['question'], row['word_list'], row['char_list'], row['aspect']\n",
    "        start, end = row['from'], row['to']\n",
    "\n",
    "        char_input_l.append(list(map(lambda x: char_vocab.get(x, len(char_vocab)+1), char_list[:end])))\n",
    "        char_input_r.append(list(map(lambda x: char_vocab.get(x, len(char_vocab)+1), char_list[start:])))\n",
    "        char_input_r_with_pad.append([char_vocab.get(char_list[i], len(char_vocab)+1) if i >= start else 0\n",
    "                                      for i in range(len(char_list))])  # replace left sequence with 0\n",
    "        _char_mask = [1] * len(char_list)\n",
    "        _char_mask[start:end] = [0.5] * (end-start)     # 1 for context, 0.5 for aspect\n",
    "        char_mask.append(_char_mask)\n",
    "        _pos_input, _offset_input = get_loc_info(char_list, start, end)\n",
    "        char_pos_input.append(_pos_input)\n",
    "        char_offset_input.append(_offset_input)\n",
    "\n",
    "        word_list_l = word_cut_func(text[:start])\n",
    "        word_list_r = word_cut_func(text[end:])\n",
    "        start = len(word_list_l)\n",
    "        end = len(word_list) - len(word_list_r)\n",
    "        if word_list[start:end] != word_cut_func(aspect):\n",
    "            if word_list[start-1:end] == word_cut_func(aspect):\n",
    "                start -= 1\n",
    "            elif word_list[start:end+1] == word_cut_func(aspect):\n",
    "                end += 1\n",
    "                print(\"Found -----------------------------------\")\n",
    "            else:\n",
    "                print(('Can not find aspect `{}` in `{}`, word list : `{}`'.format(aspect, text, word_list)))\n",
    "                #raise Exception('Can not find aspect `{}` in `{}`, word list : `{}`'.format(aspect, text, word_list))\n",
    "        word_input_l.append(list(map(lambda x: word_vocab.get(x, len(word_vocab)+1), word_list[:end])))\n",
    "        word_input_r.append(list(map(lambda x: word_vocab.get(x, len(word_vocab)+1), word_list[start:])))\n",
    "        word_input_r_with_pad.append([word_vocab.get(word_list[i], len(word_vocab) + 1) if i >= start else 0\n",
    "                                      for i in range(len(word_list))])     \n",
    "        _word_mask = [1] * len(word_list)\n",
    "        _word_mask[start:end] = [0.5] * (end - start)  \n",
    "        word_mask.append(_word_mask)\n",
    "        _pos_input, _offset_input = get_loc_info(word_list, start, end)\n",
    "        word_pos_input.append(_pos_input)\n",
    "        word_offset_input.append(_offset_input)\n",
    "    return (word_input_l, word_input_r, word_input_r_with_pad, word_mask, word_pos_input, word_offset_input,\n",
    "            char_input_l, char_input_r, char_input_r_with_pad, char_mask, char_pos_input, char_offset_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(folder, word_cut_func, is_en):\n",
    "    print('preprocessing: ', folder)\n",
    "    train_data = pd.read_csv(os.path.join(folder, 'train.csv'), header=0, index_col=None)\n",
    "    train_data['word_list'] = train_data['question'].apply(word_cut_func)\n",
    "    train_data['char_list'] = train_data['question'].apply(lambda x: list(x))\n",
    "    train_data['aspect_word_list'] = train_data['aspect'].apply(word_cut_func)\n",
    "    train_data['aspect_char_list'] = train_data['aspect'].apply(lambda x: list(x))\n",
    "\n",
    "    test_data = pd.read_csv(os.path.join(folder, 'test.csv'), header=0, index_col=None)\n",
    "    test_data['word_list'] = test_data['question'].apply(word_cut_func)\n",
    "    test_data['char_list'] = test_data['question'].apply(lambda x: list(x))\n",
    "    test_data['aspect_word_list'] = test_data['aspect'].apply(word_cut_func)\n",
    "    test_data['aspect_char_list'] = test_data['aspect'].apply(lambda x: list(x))\n",
    "\n",
    "    print('size of training set:', len(train_data))\n",
    "    print('size of test set:', len(test_data))\n",
    "\n",
    "    word_corpus = np.concatenate((train_data['word_list'].values, \n",
    "                                  test_data['word_list'].values)).tolist()\n",
    "    char_corpus = np.concatenate((train_data['char_list'].values,\n",
    "                                  test_data['char_list'].values)).tolist()\n",
    "    aspect_corpus = np.concatenate((train_data['aspect'].values, \n",
    "                                    test_data['aspect'].values)).tolist()\n",
    "    aspect_text_word_corpus = np.concatenate((train_data['aspect_word_list'].values,\n",
    "                                              test_data['aspect_word_list'].values)).tolist()\n",
    "    aspect_text_char_corpus = np.concatenate((train_data['aspect_char_list'].values,\n",
    "                                              test_data['aspect_char_list'].values)).tolist()\n",
    "\n",
    "    # build vocabulary\n",
    "    print('building vocabulary...')\n",
    "    word_vocab = build_vocabulary(word_corpus, start_id=1)\n",
    "    char_vocab = build_vocabulary(char_corpus, start_id=1)\n",
    "    aspect_vocab = build_vocabulary(aspect_corpus, start_id=0)\n",
    "    aspect_text_word_vocab = build_vocabulary(aspect_text_word_corpus, start_id=1)\n",
    "    aspect_text_char_vocab = build_vocabulary(aspect_text_char_corpus, start_id=1)\n",
    "    pickle_dump(word_vocab, os.path.join(folder, 'word_vocab.pkl'))\n",
    "    pickle_dump(char_vocab, os.path.join(folder, 'char_vocab.pkl'))\n",
    "    pickle_dump(aspect_vocab, os.path.join(folder, 'aspect_vocab.pkl'))\n",
    "    pickle_dump(aspect_text_word_vocab, os.path.join(folder, 'aspect_text_word_vocab.pkl'))\n",
    "    pickle_dump(aspect_text_char_vocab, os.path.join(folder, 'aspect_text_char_vocab.pkl'))\n",
    "    print('finished building vocabulary!')\n",
    "    print('len of word vocabulary:', len(word_vocab))\n",
    "    print('sample of word vocabulary:', list(word_vocab.items())[:10])\n",
    "    print('len of char vocabulary:', len(char_vocab))\n",
    "    print('sample of char vocabulary:', list(char_vocab.items())[:10])\n",
    "    print('len of aspect vocabulary:', len(aspect_vocab))\n",
    "    print('sample of aspect vocabulary:', list(aspect_vocab.items())[:10])\n",
    "    print('len of aspect text word vocabulary:', len(aspect_text_word_vocab))\n",
    "    print('sample of aspect text word vocabulary:', list(aspect_text_word_vocab.items())[:10])\n",
    "    print('len of aspect text char vocabulary:', len(aspect_text_char_vocab))\n",
    "    print('sample of aspect text char vocabulary:', list(aspect_text_char_vocab.items())[:10])\n",
    "\n",
    "    # prepare embedding\n",
    "    print('preparing embedding...')\n",
    "    word_w2v = build_embedding(word_corpus, word_vocab, config.word_embed_dim)\n",
    "    aspect_word_w2v = build_asp_embedding(aspect_vocab, word_cut_func, word_vocab, word_w2v)\n",
    "    aspect_text_word_w2v = build_asp_text_embedding(aspect_text_word_vocab, word_vocab, word_w2v)\n",
    "    char_w2v = build_embedding(char_corpus, char_vocab, config.word_embed_dim)\n",
    "    aspect_char_w2v = build_asp_embedding(aspect_vocab, lambda x: list(x), char_vocab, char_w2v)\n",
    "    aspect_text_char_w2v = build_asp_text_embedding(aspect_text_char_vocab, char_vocab, char_w2v)\n",
    "    np.save(os.path.join(folder, 'word_w2v.npy'), word_w2v)\n",
    "    np.save(os.path.join(folder, 'aspect_word_w2v.npy'), aspect_word_w2v)\n",
    "    np.save(os.path.join(folder, 'aspect_text_word_w2v.npy'), aspect_text_word_w2v)\n",
    "    np.save(os.path.join(folder, 'char_w2v.npy'), char_w2v)\n",
    "    np.save(os.path.join(folder, 'aspect_char_w2v.npy'), aspect_char_w2v)\n",
    "    np.save(os.path.join(folder, 'aspect_text_char_w2v.npy'), aspect_text_char_w2v)\n",
    "\n",
    "    print('finished preparing embedding!')\n",
    "    print('shape of word_w2v:', word_w2v.shape)\n",
    "    print('sample of word_w2v:', word_w2v[:2, :5])\n",
    "    print('shape of char_w2v:', char_w2v.shape)\n",
    "    print('sample of char_w2v:', char_w2v[:2, :5])\n",
    "    print('shape of aspect_word_w2v:', aspect_word_w2v.shape)\n",
    "    print('sample of aspect_word_w2v:', aspect_word_w2v[:2, :5])\n",
    "    print('shape of aspect_char_w2v:', aspect_char_w2v.shape)\n",
    "    print('sample of aspect_char_w2v:', aspect_char_w2v[:2, :5])\n",
    "    print('shape of aspect_text_word_w2v:', aspect_text_word_w2v.shape)\n",
    "    print('sample of aspect_text_word_w2v:', aspect_text_word_w2v[:2, :5])\n",
    "    print('shape of aspect_text_char_w2v:', aspect_text_char_w2v.shape)\n",
    "    print('sample of aspect_text_char_w2v:', aspect_text_char_w2v[:2, :5])\n",
    "\n",
    "    if is_en:\n",
    "        word_glove = build_glove_embedding(word_vocab, glove_vectors, glove_embed_dim)\n",
    "        aspect_word_glove = build_asp_embedding(aspect_vocab, word_cut_func, word_vocab, word_glove)\n",
    "        aspect_text_word_glove = build_asp_text_embedding(aspect_text_word_vocab, word_vocab, word_glove)\n",
    "        np.save(os.path.join(folder, 'word_glove.npy'), word_glove)\n",
    "        np.save(os.path.join(folder, 'aspect_word_glove.npy'), aspect_word_glove)\n",
    "        np.save(os.path.join(folder, 'aspect_text_word_glove.npy'), aspect_text_word_glove)\n",
    "        print('shape of word_glove:', word_glove.shape)\n",
    "        print('sample of word_glove:', word_glove[:2, :5])\n",
    "        print('shape of aspect_word_glove:', aspect_word_glove.shape)\n",
    "        print('sample of aspect_word_glove:', aspect_word_glove[:2, :5])\n",
    "        print('shape of aspect_text_word_glove:', aspect_text_word_glove.shape)\n",
    "        print('sample of aspect_text_word_glove:', aspect_text_word_glove[:2, :5])\n",
    "\n",
    "    # prepare input\n",
    "    print('preparing text input...')\n",
    "    train_word_input = train_data['word_list'].apply(\n",
    "        lambda x: [word_vocab.get(word, len(word_vocab)+1) for word in x]).values.tolist()\n",
    "    train_char_input = train_data['char_list'].apply(\n",
    "        lambda x: [char_vocab.get(char, len(char_vocab)+1) for char in x]).values.tolist()\n",
    "    test_word_input = test_data['word_list'].apply(\n",
    "        lambda x: [word_vocab.get(word, len(word_vocab)+1) for word in x]).values.tolist()\n",
    "    test_char_input = test_data['char_list'].apply(\n",
    "        lambda x: [char_vocab.get(char, len(char_vocab)+1) for char in x]).values.tolist()\n",
    "    pickle_dump(train_word_input, os.path.join(folder, 'train_word_input.pkl'))\n",
    "    pickle_dump(train_char_input, os.path.join(folder, 'train_char_input.pkl'))\n",
    "    pickle_dump(test_word_input, os.path.join(folder, 'test_word_input.pkl'))\n",
    "    pickle_dump(test_char_input, os.path.join(folder, 'test_char_input.pkl'))\n",
    "    print('finished preparing text input!')\n",
    "    print('length analysis of text word input:')\n",
    "    analyze_len_distribution(train_word_input, test_word_input)\n",
    "    print('length analysis of text char input')\n",
    "    analyze_len_distribution(train_char_input, test_char_input)\n",
    "\n",
    "    print('preparing aspect input...')\n",
    "    train_aspect_input = train_data['aspect'].apply(lambda x: [aspect_vocab[x]]).values.tolist()\n",
    "    test_aspect_input = test_data['aspect'].apply(lambda x: [aspect_vocab[x]]).values.tolist()\n",
    "    pickle_dump(train_aspect_input, os.path.join(folder, 'train_aspect_input.pkl'))\n",
    "    pickle_dump(test_aspect_input, os.path.join(folder, 'test_aspect_input.pkl'))\n",
    "    print('finished preparing aspect input!')\n",
    "\n",
    "    print('preparing aspect text input...')\n",
    "    train_aspect_text_word_input = train_data['aspect_word_list'].apply(\n",
    "        lambda x: [aspect_text_word_vocab.get(word, len(aspect_text_word_vocab) + 1) for word in x]).values.tolist()\n",
    "    train_aspect_text_char_input = train_data['aspect_char_list'].apply(\n",
    "        lambda x: [aspect_text_char_vocab.get(char, len(aspect_text_char_vocab) + 1) for char in x]).values.tolist()\n",
    "    test_aspect_text_word_input = test_data['aspect_word_list'].apply(\n",
    "        lambda x: [aspect_text_word_vocab.get(word, len(aspect_text_word_vocab) + 1) for word in x]).values.tolist()\n",
    "    test_aspect_text_char_input = test_data['aspect_char_list'].apply(\n",
    "        lambda x: [aspect_text_char_vocab.get(char, len(aspect_text_char_vocab) + 1) for char in x]).values.tolist()\n",
    "    pickle_dump(train_aspect_text_word_input, os.path.join(folder, 'train_word_aspect_input.pkl'))\n",
    "    pickle_dump(train_aspect_text_char_input, os.path.join(folder, 'train_char_aspect_input.pkl'))\n",
    "    \n",
    "    \n",
    "    pickle_dump(test_aspect_text_word_input, os.path.join(folder, 'test_word_aspect_input.pkl'))\n",
    "    \n",
    "    print('finished preparing aspect text input!')\n",
    "    print('length analysis of aspect text word input:')\n",
    "    analyze_len_distribution(train_aspect_text_word_input, test_aspect_text_word_input)\n",
    "    print('length analysis of aspect text char input')\n",
    "    analyze_len_distribution(train_aspect_text_char_input, test_aspect_text_char_input)\n",
    "\n",
    "    if 'from' in train_data.columns:\n",
    "        print('preparing left text input, right text input & position input...')\n",
    "        train_word_input_l, train_word_input_r, train_word_input_r_with_pad, train_word_mask, train_word_pos_input, \\\n",
    "            train_word_offset_input, train_char_input_l, train_char_input_r, train_char_input_r_with_pad, \\\n",
    "            train_char_mask, train_char_pos_input, train_char_offset_input = split_and_get_loc_info(train_data,\n",
    "                                                                                                         word_vocab,\n",
    "                                                                                                         char_vocab,\n",
    "                                                                                                         word_cut_func)\n",
    "        pickle_dump(train_word_input_l, os.path.join(folder, 'train_word_input_l.pkl'))\n",
    "        pickle_dump(train_word_input_r, os.path.join(folder, 'train_word_input_r.pkl'))\n",
    "        pickle_dump(train_word_input_r_with_pad, os.path.join(folder, 'train_word_input_r_with_pad.pkl'))\n",
    "        pickle_dump(train_word_mask, os.path.join(folder, 'train_word_mask.pkl'))\n",
    "        pickle_dump(train_word_pos_input, os.path.join(folder, 'train_word_pos_input.pkl'))\n",
    "        pickle_dump(train_word_offset_input, os.path.join(folder, 'train_word_offset_input.pkl'))\n",
    "        pickle_dump(train_char_input_l, os.path.join(folder, 'train_char_input_l.pkl'))\n",
    "        pickle_dump(train_char_input_r, os.path.join(folder, 'train_char_input_r.pkl'))\n",
    "        pickle_dump(train_char_input_r_with_pad, os.path.join(folder, 'train_char_input_r_with_pad.pkl'))\n",
    "        pickle_dump(train_char_mask, os.path.join(folder, 'train_char_mask.pkl'))\n",
    "        pickle_dump(train_char_pos_input, os.path.join(folder, 'train_char_pos_input.pkl'))\n",
    "        pickle_dump(train_char_offset_input, os.path.join(folder, 'train_char_offset_input.pkl'))\n",
    "\n",
    "\n",
    "        test_word_input_l, test_word_input_r, test_word_input_r_with_pad, test_word_mask, test_word_pos_input, \\\n",
    "            test_word_offset_input, test_char_input_l, test_char_input_r, test_char_input_r_with_pad, test_char_mask, \\\n",
    "            test_char_pos_input, test_char_offset_input = split_and_get_loc_info(test_data, word_vocab,\n",
    "                                                                                      char_vocab, word_cut_func)\n",
    "        pickle_dump(test_word_input_l, os.path.join(folder, 'test_word_input_l.pkl'))\n",
    "        pickle_dump(test_word_input_r, os.path.join(folder, 'test_word_input_r.pkl'))\n",
    "        pickle_dump(test_word_input_r_with_pad, os.path.join(folder, 'test_word_input_r_with_pad.pkl'))\n",
    "        pickle_dump(test_word_mask, os.path.join(folder, 'test_word_mask.pkl'))\n",
    "        pickle_dump(test_word_pos_input, os.path.join(folder, 'test_word_pos_input.pkl'))\n",
    "        pickle_dump(test_word_offset_input, os.path.join(folder, 'test_word_offset_input.pkl'))\n",
    "        pickle_dump(test_char_input_l, os.path.join(folder, 'test_char_input_l.pkl'))\n",
    "        pickle_dump(test_char_input_r, os.path.join(folder, 'test_char_input_r.pkl'))\n",
    "        pickle_dump(test_char_input_r_with_pad, os.path.join(folder, 'test_char_input_r_with_pad.pkl'))\n",
    "        pickle_dump(test_char_mask, os.path.join(folder, 'test_char_mask.pkl'))\n",
    "        pickle_dump(test_char_pos_input, os.path.join(folder, 'test_char_pos_input.pkl'))\n",
    "        pickle_dump(test_char_offset_input, os.path.join(folder, 'test_char_offset_input.pkl'))\n",
    "\n",
    "        print('length analysis of left text word input:')\n",
    "        analyze_len_distribution(train_word_input_l, test_word_input_l)\n",
    "        print('length analysis of left text char input')\n",
    "        analyze_len_distribution(train_char_input_l, test_char_input_l)\n",
    "        print('length analysis of right text word input:')\n",
    "        analyze_len_distribution(train_word_input_r, test_word_input_r)\n",
    "        print('length analysis of right text char input')\n",
    "        analyze_len_distribution(train_char_input_r, test_char_input_r)\n",
    "\n",
    "    # prepare output\n",
    "    print('preparing output....')\n",
    "    pickle_dump(train_data['sentiment'].values.tolist(), os.path.join(folder, 'train_label.pkl'))\n",
    "    \n",
    "    if 'sentiment' in test_data.columns:\n",
    "        pickle_dump(test_data['sentiment'].values.tolist(), os.path.join(folder, 'test_label.pkl'))\n",
    "    print('finished preparing output!')\n",
    "    print('class analysis of training set:')\n",
    "    analyze_class_distribution(train_data['sentiment'].values.tolist())\n",
    "\n",
    "    if 'sentiment' in test_data.columns:\n",
    "        print('class analysis of test set:')\n",
    "        analyze_class_distribution(test_data['sentiment'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "glove_vectors, glove_embed_dim = load_glove('data/glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing:  data\n",
      "size of training set: 22428\n",
      "size of test set: 9612\n",
      "building vocabulary...\n",
      "finished building vocabulary!\n",
      "len of word vocabulary: 20036\n",
      "sample of word vocabulary: [('portability', 1), ('la', 2), ('islampur', 3), ('historic', 4), ('chandiroor', 5), ('discoveries', 6), ('cum', 7), ('u+0080', 8), ('librarian', 9), ('netaji', 10)]\n",
      "len of char vocabulary: 55\n",
      "sample of char vocabulary: [(' ', 1), ('k', 2), ('h', 3), ('f', 4), ('<', 5), ('+', 6), ('!', 7), ('i', 8), ('m', 9), ('^', 10)]\n",
      "len of aspect vocabulary: 480\n",
      "sample of aspect vocabulary: [('rbi', 0), ('rape', 1), ('israel', 2), ('paris', 3), ('legislative', 4), ('planes', 5), ('edible', 6), ('reactor', 7), ('road', 8), ('yojana', 9)]\n",
      "len of aspect text word vocabulary: 480\n",
      "sample of aspect text word vocabulary: [('rbi', 1), ('rape', 2), ('israel', 3), ('paris', 4), ('legislative', 5), ('planes', 6), ('edible', 7), ('reactor', 8), ('road', 9), ('yojana', 10)]\n",
      "len of aspect text char vocabulary: 26\n",
      "sample of aspect text char vocabulary: [('k', 1), ('h', 2), ('f', 3), ('i', 4), ('m', 5), ('z', 6), ('d', 7), ('t', 8), ('p', 9), ('s', 10)]\n",
      "preparing embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding out of vocabulary： 0\n",
      "aspect embedding out of vocabulary: 0\n",
      "aspect text embedding out of vocabulary: 0\n",
      "embedding out of vocabulary： 0\n",
      "aspect embedding out of vocabulary: 0\n",
      "aspect text embedding out of vocabulary: 0\n",
      "finished preparing embedding!\n",
      "shape of word_w2v: (20038, 300)\n",
      "sample of word_w2v: [[ 0.          0.          0.          0.          0.        ]\n",
      " [-0.18587156 -0.5147417  -0.06109993 -0.12709035  0.57431316]]\n",
      "shape of char_w2v: (57, 300)\n",
      "sample of char_w2v: [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.04808243  0.03608881 -0.04355727 -0.03675759 -0.14874372]]\n",
      "shape of aspect_word_w2v: (480, 300)\n",
      "sample of aspect_word_w2v: [[ 0.20162572 -0.20056358  0.29159245 -0.28464085 -0.14836526]\n",
      " [-0.36700869 -0.37976828 -0.404596   -0.29997861 -0.1297816 ]]\n",
      "shape of aspect_char_w2v: (480, 300)\n",
      "sample of aspect_char_w2v: [[ 0.03570366  0.0483878   0.00361275 -0.03691958 -0.03800088]\n",
      " [ 0.01887818  0.08865483 -0.01886964 -0.06716267 -0.05968153]]\n",
      "shape of aspect_text_word_w2v: (482, 300)\n",
      "sample of aspect_text_word_w2v: [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.20162572 -0.20056358  0.29159245 -0.28464085 -0.14836526]]\n",
      "shape of aspect_text_char_w2v: (28, 300)\n",
      "sample of aspect_text_char_w2v: [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.01787679  0.00838442 -0.04696614  0.04233711 -0.04068244]]\n",
      "glove embedding out of vocabulary： 2414\n",
      "aspect embedding out of vocabulary: 0\n",
      "aspect text embedding out of vocabulary: 0\n",
      "shape of word_glove: (20038, 300)\n",
      "sample of word_glove: [[ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.26631 -0.32194  0.55456 -0.10179  1.0839 ]]\n",
      "shape of aspect_word_glove: (480, 300)\n",
      "sample of aspect_word_glove: [[-0.050914    0.35534999  0.11039     0.41079    -0.27181   ]\n",
      " [ 0.090373    0.42377001 -0.6085      0.34022    -0.46693999]]\n",
      "shape of aspect_text_word_glove: (482, 300)\n",
      "sample of aspect_text_word_glove: [[ 0.        0.        0.        0.        0.      ]\n",
      " [-0.050914  0.35535   0.11039   0.41079  -0.27181 ]]\n",
      "preparing text input...\n",
      "finished preparing text input!\n",
      "length analysis of text word input:\n",
      "max len: 306 min_len 4 avg len 24.491666666666667 median len 23.0\n",
      "23 0.5248751560549313\n",
      "28 0.7243757802746567\n",
      "33 0.8480024968789014\n",
      "38 0.920692883895131\n",
      "43 0.9578339575530587\n",
      "48 0.9775593008739076\n",
      "53 0.9878277153558053\n",
      "58 0.9932896379525593\n",
      "length analysis of text char input\n",
      "max len: 1767 min_len 26 avg len 146.4518102372035 median len 137.0\n",
      "137 0.5061485642946317\n",
      "142 0.5416666666666666\n",
      "147 0.5779338327091136\n",
      "152 0.6141385767790262\n",
      "157 0.6471598002496879\n",
      "162 0.6772159800249687\n",
      "167 0.7059925093632958\n",
      "172 0.7336766541822721\n",
      "177 0.7581772784019976\n",
      "182 0.7807740324594257\n",
      "187 0.8007178526841449\n",
      "192 0.8194756554307117\n",
      "197 0.836641697877653\n",
      "202 0.8534332084893883\n",
      "207 0.8675717852684145\n",
      "212 0.8811173533083645\n",
      "217 0.8936953807740324\n",
      "222 0.904338327091136\n",
      "227 0.9136079900124844\n",
      "232 0.9223158551810238\n",
      "237 0.9301498127340824\n",
      "242 0.9379837702871411\n",
      "247 0.9443820224719102\n",
      "252 0.9500624219725343\n",
      "257 0.9555243445692884\n",
      "262 0.960143570536829\n",
      "267 0.9642009987515605\n",
      "272 0.9675093632958801\n",
      "277 0.9706928838951311\n",
      "282 0.9730337078651685\n",
      "287 0.9758739076154807\n",
      "292 0.9783707865168539\n",
      "297 0.9805243445692884\n",
      "302 0.9827403245942572\n",
      "307 0.9844569288389513\n",
      "312 0.9860799001248439\n",
      "317 0.9871722846441947\n",
      "322 0.9882646691635456\n",
      "327 0.9892009987515605\n",
      "332 0.9902933832709113\n",
      "preparing aspect input...\n",
      "finished preparing aspect input!\n",
      "preparing aspect text input...\n",
      "finished preparing aspect text input!\n",
      "length analysis of aspect text word input:\n",
      "max len: 1 min_len 1 avg len 1.0 median len 1.0\n",
      "length analysis of aspect text char input\n",
      "max len: 15 min_len 2 avg len 5.944818976279651 median len 5.0\n",
      "5 0.5144506866416979\n",
      "10 0.9698189762796504\n",
      "preparing left text input, right text input & position input...\n",
      "length analysis of left text word input:\n",
      "max len: 274 min_len 1 avg len 13.59812734082397 median len 12.0\n",
      "12 0.5030898876404495\n",
      "17 0.7395131086142323\n",
      "22 0.8848314606741573\n",
      "27 0.9512172284644195\n",
      "32 0.9798377028714107\n",
      "37 0.9916978776529338\n",
      "length analysis of left text char input\n",
      "max len: 1559 min_len 3 avg len 81.12771535580525 median len 74.0\n",
      "74 0.5035268414481897\n",
      "79 0.5482833957553058\n",
      "84 0.5908239700374532\n",
      "89 0.630649188514357\n",
      "94 0.6685705368289638\n",
      "99 0.7049625468164794\n",
      "104 0.7388888888888889\n",
      "109 0.7696004993757802\n",
      "114 0.7982209737827716\n",
      "119 0.8216604244694132\n",
      "124 0.8426966292134831\n",
      "129 0.8634519350811486\n",
      "134 0.880056179775281\n",
      "139 0.8956616729088639\n",
      "144 0.9099875156054932\n",
      "149 0.9216292134831461\n",
      "154 0.9310861423220974\n",
      "159 0.9397627965043696\n",
      "164 0.9476903870162298\n",
      "169 0.9539013732833957\n",
      "174 0.9601123595505618\n",
      "179 0.9651685393258427\n",
      "184 0.9699438202247191\n",
      "189 0.9747815230961299\n",
      "194 0.9784644194756554\n",
      "199 0.9814606741573034\n",
      "204 0.984019975031211\n",
      "209 0.9857365792759051\n",
      "214 0.9877028714107365\n",
      "219 0.9895755305867665\n",
      "224 0.9907615480649189\n",
      "length analysis of right text word input:\n",
      "max len: 130 min_len 1 avg len 11.893539325842697 median len 10.0\n",
      "10 0.5297128589263421\n",
      "15 0.716354556803995\n",
      "20 0.8452247191011236\n",
      "25 0.9189138576779026\n",
      "30 0.959956304619226\n",
      "35 0.9790262172284644\n",
      "40 0.9893570536828964\n",
      "45 0.9942571785268415\n",
      "length analysis of right text char input\n",
      "max len: 701 min_len 2 avg len 71.2689138576779 median len 59.0\n",
      "59 0.5043695380774033\n",
      "64 0.5403245942571785\n",
      "69 0.5749375780274657\n",
      "74 0.6081460674157303\n",
      "79 0.6389200998751561\n",
      "84 0.668414481897628\n",
      "89 0.6966916354556804\n",
      "94 0.7225967540574282\n",
      "99 0.746441947565543\n",
      "104 0.7690074906367041\n",
      "109 0.7895755305867665\n",
      "114 0.8089263420724095\n",
      "119 0.8289013732833957\n",
      "124 0.8464731585518103\n",
      "129 0.8612359550561798\n",
      "134 0.8755305867665418\n",
      "139 0.8878277153558053\n",
      "144 0.9004681647940075\n",
      "149 0.910518102372035\n",
      "154 0.9199438202247191\n",
      "159 0.9289013732833957\n",
      "164 0.9367977528089888\n",
      "169 0.9432584269662921\n",
      "174 0.9492821473158551\n",
      "179 0.9548689138576779\n",
      "184 0.960143570536829\n",
      "189 0.9645131086142322\n",
      "194 0.9682584269662922\n",
      "199 0.9714107365792759\n",
      "204 0.9741260923845193\n",
      "209 0.9770287141073658\n",
      "214 0.9797128589263421\n",
      "219 0.981772784019975\n",
      "224 0.9837702871410736\n",
      "229 0.9855493133583021\n",
      "234 0.9873907615480649\n",
      "239 0.9887328339575531\n",
      "244 0.9900436953807741\n",
      "preparing output....\n",
      "finished preparing output!\n",
      "class analysis of training set:\n",
      "1 11561 0.5154717317638666\n",
      "2 5985 0.26685393258426965\n",
      "0 4882 0.21767433565186375\n",
      "class analysis of test set:\n",
      "1 4976 0.5176862255513941\n",
      "2 2536 0.2638368705784436\n",
      "0 2100 0.2184769038701623\n"
     ]
    }
   ],
   "source": [
    "pre_process('data', lambda x: nltk.word_tokenize(x), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
