{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    " e: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None, W_constraint=None,\n",
    "                 u_constraint=None, b_constraint=None, use_W=True, use_bias=False, return_self_attend=False,\n",
    "                 return_attend_weight=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.use_W = use_W\n",
    "        self.use_bias = use_bias\n",
    "        self.return_self_attend = return_self_attend    # whether perform self attention and return it\n",
    "        self.return_attend_weight = return_attend_weight    # whether return attention weight\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        if self.use_W:\n",
    "            self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),  initializer=self.init,\n",
    "                                     name='{}_W'.format(self.name), regularizer=self.W_regularizer,\n",
    "                                     constraint=self.W_constraint)\n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],), initializer='zero', name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer, constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],), initializer=self.init, name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer, constraint=self.u_constraint)\n",
    "        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.use_W:\n",
    "            x = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ait = Attention.dot_product(x, self.u)\n",
    "        if self.use_bias:\n",
    "            ait += self.b\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        if self.return_self_attend:\n",
    "            attend_output = K.sum(x * K.expand_dims(a), axis=1)\n",
    "            if self.return_attend_weight:\n",
    "                return [attend_output, a]\n",
    "            else:\n",
    "                return attend_output\n",
    "        else:\n",
    "            return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_self_attend:\n",
    "            if self.return_attend_weight:\n",
    "                return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\n",
    "            else:\n",
    "                return input_shape[0], input_shape[-1]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def dot_product(x, kernel):\n",
    "        \"\"\"\n",
    "        Wrapper for dot product operation, in order to be compatible with both\n",
    "        Theano and Tensorflow\n",
    "        Args:\n",
    "            x (): input\n",
    "            kernel (): weights\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "        else:\n",
    "            return K.dot(x, kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAttention(Layer):\n",
    "    \"\"\"\n",
    "    Multiple attentions non-linearly combined with a recurrent neural network (gru) .\n",
    "    Supports Masking.\n",
    "    Follows the work of Peng et al. [http://aclweb.org/anthology/D17-1047]\n",
    "    \"Recurrent Attention Network on Memory for Aspect Sentiment Analysis\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, n_hop=5, return_attend_weight=False, initializer='orthogonal', regularizer=None,\n",
    "                 constraint=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.n_hop = n_hop\n",
    "        self.return_attend_weight = return_attend_weight\n",
    "\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.constraint = constraints.get(constraint)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        super(RecurrentAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):   # input: memory(3D) & aspect(2D)\n",
    "            input_mem_shape = input_shape[0]\n",
    "            al_w_shape = input_shape[0][-1] + input_shape[1][-1] + self.units\n",
    "        else:   # input: just memory\n",
    "            input_mem_shape = input_shape\n",
    "            al_w_shape = input_shape[-1] + self.units\n",
    "\n",
    "        # attention weights\n",
    "        self.al_w = self.add_weight(shape=(self.n_hop, al_w_shape, 1), initializer=self.initializer,\n",
    "                                    name='{}_al_w'.format(self.name), regularizer=self.regularizer,\n",
    "                                    constraint=self.constraint)\n",
    "        self.al_b = self.add_weight(shape=(self.n_hop, 1), initializer='zero', name='{}_al_b'.format(self.name),\n",
    "                                    regularizer=self.regularizer, constraint=self.constraint)\n",
    "\n",
    "        # gru weights\n",
    "        self.gru_wr = self.add_weight(shape=(input_mem_shape[-1], self.units), initializer=self.initializer,\n",
    "                                      name='{}_wr'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.gru_ur = self.add_weight(shape=(self.units, self.units), initializer=self.initializer,\n",
    "                                      name='{}_ur'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.gru_wz = self.add_weight(shape=(input_mem_shape[-1], self.units), initializer=self.initializer,\n",
    "                                      name='{}_wz'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.gru_uz = self.add_weight(shape=(self.units, self.units), initializer=self.initializer,\n",
    "                                      name='{}_uz'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.gru_wx = self.add_weight(shape=(input_mem_shape[-1], self.units), initializer=self.initializer,\n",
    "                                      name='{}_wx'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.gru_wg = self.add_weight(shape=(self.units, self.units), initializer=self.initializer,\n",
    "                                      name='{}_wg'.format(self.name), regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        super(RecurrentAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if isinstance(inputs, list):\n",
    "            memory, aspect = inputs\n",
    "            mask = mask[0]\n",
    "        else:\n",
    "            memory = inputs\n",
    "\n",
    "        attend_weights = []\n",
    "        batch_size = K.shape(memory)[0]\n",
    "        time_steps = K.shape(memory)[1]\n",
    "        e = K.zeros(shape=(batch_size, self.units))\n",
    "        for h in range(self.n_hop):\n",
    "            # compute attention weight\n",
    "            repeat_e = K.repeat(e, time_steps)\n",
    "            if isinstance(inputs, list):\n",
    "                repeat_asp = K.repeat(aspect, time_steps)\n",
    "                inputs_concat = K.concatenate([memory, repeat_asp, repeat_e], axis=-1)\n",
    "            else:\n",
    "                inputs_concat = K.concatenate([memory, repeat_e], axis=-1)\n",
    "            g = K.squeeze(K.dot(inputs_concat, self.al_w[h]), axis=-1) + self.al_b[h]   # [batch_size, time_steps]\n",
    "            a = K.exp(g)\n",
    "\n",
    "            # apply mask after the exp. will be re-normalized next\n",
    "            if mask is not None:\n",
    "                a *= K.cast(mask, K.floatx())\n",
    "\n",
    "            a /= K.cast(K.sum(a, axis=-1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "            attend_weights.append(a)\n",
    "\n",
    "            # apply attention\n",
    "            a_expand = K.expand_dims(a)    # [batch_size, time_steps, 1]\n",
    "            i_AL = K.sum(memory * a_expand, axis=1)   # [batch_size, hidden], i_AL is the input of gru at time `h`\n",
    "\n",
    "            # gru implementation\n",
    "            r = K.sigmoid(K.dot(i_AL, self.gru_wr) + K.dot(e, self.gru_ur))    # reset gate\n",
    "            z = K.sigmoid(K.dot(i_AL, self.gru_wz) + K.dot(e, self.gru_uz))    # update gate\n",
    "            _e = K.tanh(K.dot(i_AL, self.gru_wx) + K.dot(r*e, self.gru_wg))\n",
    "            e = (1 - z) * e + z * _e  # update e\n",
    "\n",
    "        if self.return_attend_weight:\n",
    "            return [e, K.concatenate(attend_weights, axis=0)]\n",
    "        else:\n",
    "            return e\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            memory_shape = input_shape[0]\n",
    "        else:\n",
    "            memory_shape = input_shape\n",
    "        if self.return_attend_weight:\n",
    "            return [(memory_shape[0], self.units), (self.n_hop, memory_shape[0], memory_shape[1])]\n",
    "        else:\n",
    "            return memory_shape[0], self.units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveAttention(Layer):\n",
    "    \"\"\"\n",
    "    Interactive attention between context and aspect text.\n",
    "    Supporting Masking.\n",
    "    Follows the work of Dehong et al. [https://www.ijcai.org/proceedings/2017/0568.pdf]\n",
    "    \"Interactive Attention Networks for Aspect-Level Sentiment Classification\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attend_weight=False, initializer='orthogonal', regularizer=None,\n",
    "                 constraint=None, **kwargs):\n",
    "        self.return_attend_weight = return_attend_weight\n",
    "\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.constraint = constraints.get(constraint)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        super(InteractiveAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        context_shape, asp_text_shape = input_shape\n",
    "\n",
    "        self.context_w = self.add_weight(shape=(context_shape[-1], asp_text_shape[-1]), initializer=self.initializer,\n",
    "                                         regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                         name='{}_context_w'.format(self.name))\n",
    "        self.context_b = self.add_weight(shape=(context_shape[1],), initializer='zero', regularizer=self.regularizer,\n",
    "                                         constraint=self.constraint, name='{}_context_b'.format(self.name))\n",
    "        self.aspect_w = self.add_weight(shape=(asp_text_shape[-1], context_shape[-1]), initializer=self.initializer,\n",
    "                                        regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                        name='{}_aspect_w'.format(self.name))\n",
    "        self.aspect_b = self.add_weight(shape=(asp_text_shape[1],), initializer='zero', regularizer=self.regularizer,\n",
    "                                        constraint=self.constraint, name='{}_aspect_b'.format(self.name))\n",
    "\n",
    "        super(InteractiveAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        assert isinstance(inputs, list)\n",
    "        if mask is not None:\n",
    "            context_mask, asp_text_mask = mask\n",
    "        else:\n",
    "            context_mask = None\n",
    "            asp_text_mask = None\n",
    "\n",
    "        context, asp_text = inputs\n",
    "\n",
    "        context_avg = K.mean(context, axis=1)\n",
    "        asp_text_avg = K.mean(asp_text, axis=1)\n",
    "\n",
    "        # attention over context with aspect_text\n",
    "        a_c = K.tanh(K.batch_dot(asp_text_avg, K.dot(context, self.context_w), axes=[1, 2]) + self.context_b)\n",
    "        a_c = K.exp(a_c)\n",
    "        if context_mask is not None:\n",
    "            a_c *= K.cast(context_mask, K.floatx())\n",
    "        a_c /= K.cast(K.sum(a_c, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        attend_context = K.sum(context * K.expand_dims(a_c), axis=1)\n",
    "\n",
    "        # attention over aspect text with context\n",
    "        a_t = K.tanh(K.batch_dot(context_avg, K.dot(asp_text, self.aspect_w), axes=[1, 2]) + self.aspect_b)\n",
    "        a_t = K.exp(a_t)\n",
    "        if context_mask is not None:\n",
    "            a_t *= K.cast(asp_text_mask, K.floatx())\n",
    "        a_t = K.cast(K.sum(a_t, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        attend_asp_text = K.sum(asp_text * K.expand_dims(a_t), axis=1)\n",
    "\n",
    "        attend_concat = K.concatenate([attend_context, attend_asp_text], axis=-1)\n",
    "\n",
    "        if self.return_attend_weight:\n",
    "            return [attend_concat, a_c, a_t]\n",
    "        else:\n",
    "            return attend_concat\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        context_shape, asp_text_shape = input_shape\n",
    "        if self.return_attend_weight:\n",
    "            return [(context_shape[0], context_shape[-1]+asp_text_shape[-1]), (context_shape[0], context_shape[1]),\n",
    "                    (asp_text_shape[0], asp_text_shape[1])]\n",
    "        else:\n",
    "            return context_shape[0], context_shape[-1]+asp_text_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentAttention(Layer):\n",
    "    \"\"\"\n",
    "    Sentence-level content attention.\n",
    "    Supporting Masking.\n",
    "    Follows the work of Liu et al. [https://dl.acm.org/citation.cfm?id=3186001]\n",
    "    \"Content Attention Model for Aspect Based Sentiment Analysis\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attend_weight=False, initializer='orthogonal', regularizer=None,\n",
    "                 constraint=None, **kwargs):\n",
    "        self.return_attend_weight = return_attend_weight\n",
    "\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "        self.constraint = constraints.get(constraint)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        super(ContentAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        context_shape, aspect_shape, sentence_shape = input_shape\n",
    "\n",
    "        self.context_w = self.add_weight(shape=(context_shape[-1], context_shape[-1]), initializer=self.initializer,\n",
    "                                         regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                         name='{}_context_w'.format(self.name))\n",
    "        self.aspect_w = self.add_weight(shape=(aspect_shape[-1], context_shape[-1]), initializer=self.initializer,\n",
    "                                        regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                        name='{}_aspect_w'.format(self.name))\n",
    "        self.sent_w = self.add_weight(shape=(sentence_shape[-1], context_shape[-1]), initializer=self.initializer,\n",
    "                                      regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                      name='{}_sentence_w'.format(self.name))\n",
    "\n",
    "        self.attend_w = self.add_weight(shape=(context_shape[-1], 1), initializer=self.initializer,\n",
    "                                        regularizer=self.regularizer, constraint=self.constraint,\n",
    "                                        name='{}_attend_w'.format(self.name))\n",
    "        super(ContentAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        assert isinstance(inputs, list)\n",
    "        context, aspect, sentence = inputs\n",
    "        context_mask, _, _, = mask\n",
    "        time_step = K.shape(context)[1]\n",
    "\n",
    "        repeat_aspect = K.repeat(aspect, time_step)\n",
    "        repeat_sent = K.repeat(sentence, time_step)\n",
    "\n",
    "        g = K.dot(K.tanh(K.dot(context, self.context_w) + K.dot(repeat_aspect, self.aspect_w) + K.dot(repeat_sent, self.sent_w)), self.attend_w)\n",
    "        a = K.exp(K.squeeze(g, axis=-1))\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if context_mask is not None:\n",
    "            a *= K.cast(context_mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=-1, keepdims=True) + K.epsilon(), K.floatx())     # [batch_size, time_steps]\n",
    "\n",
    "        # apply attention\n",
    "        a_expand = K.expand_dims(a)  # [batch_size, time_steps, 1]\n",
    "        attend_context = K.sum(context * a_expand, axis=1) + sentence  # [batch_size, hidden]\n",
    "\n",
    "        if self.return_attend_weight:\n",
    "            return [attend_context, a]\n",
    "        else:\n",
    "            return attend_context\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        context_shape, _, _ = input_shape\n",
    "\n",
    "        if self.return_attend_weight:\n",
    "            return [(context_shape[0], context_shape[-1]), (context_shape[0], context_shape[1])]\n",
    "        else:\n",
    "            return context_shape[0], context_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
