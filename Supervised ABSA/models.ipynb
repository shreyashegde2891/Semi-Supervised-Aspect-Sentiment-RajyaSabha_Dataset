{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from custom_layers.ipynb\n",
      "importing Jupyter notebook from utils.ipynb\n",
      "importing Jupyter notebook from dataloader.ipynb\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, SpatialDropout1D, Dropout, Conv1D, MaxPool1D, Flatten, concatenate, Dense, \\\n",
    "    LSTM, Bidirectional, Activation, MaxPooling1D, Add, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D, RepeatVector, \\\n",
    "    TimeDistributed, Permute, multiply, Lambda, add, Masking, BatchNormalization, Softmax, Reshape, ReLU, \\\n",
    "    ZeroPadding1D, subtract\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import import_ipynb\n",
    "from custom_layers import Attention, RecurrentAttention, InteractiveAttention, ContentAttention\n",
    "from utils import get_sentiment_score\n",
    "from dataloader import load_token\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for sentiment analysis model\n",
    "class SentimentModelMetrics(Callback):\n",
    "    def __init__(self):\n",
    "        super(SentimentModelMetrics, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_accs = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if len(self.validation_data[:-3]) == 1:\n",
    "            x_valid = self.validation_data[0]\n",
    "        else:\n",
    "            x_valid = self.validation_data[:-3]\n",
    "        y_valid = self.validation_data[-3]\n",
    "        valid_results = self.model.predict(x_valid)\n",
    "        _val_acc, _val_f1 = get_sentiment_score(y_valid, valid_results)\n",
    "        logs['val_acc'] = _val_acc\n",
    "        logs['val_f1'] = _val_f1\n",
    "        self.val_accs.append(_val_acc)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print('val_acc: %f' % _val_acc)\n",
    "        print('val_f1: %f' % _val_f1)\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for sentiment analysis\n",
    "class SentimentModel(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.level = self.config.level\n",
    "        \n",
    "        self.max_len = self.config.max_len[self.config.data_name][self.level]\n",
    "        self.left_max_len = self.config.left_max_len[self.config.data_name][self.level]\n",
    "        self.right_max_len = self.config.right_max_len[self.config.data_name][self.level]\n",
    "        self.asp_max_len = self.config.asp_max_len[self.config.data_name][self.level]\n",
    "\n",
    "        if self.config.use_text_input or self.config.use_text_input_l or self.config.use_text_input_r or self.config.use_text_input_r_with_pad:\n",
    "            self.text_embeddings = np.load('./data/%s/%s_%s.npy' % (self.config.data_dir, self.level,\n",
    "                                                                    self.config.word_embed_type))\n",
    "            self.config.idx2token = load_token(self.config.data_dir, self.level)\n",
    "        else:\n",
    "            self.text_embeddings = None\n",
    "        if self.config.use_aspect_input:\n",
    "            self.aspect_embeddings = np.load('./data/%s/aspect_%s_%s.npy' % (self.config.data_dir, self.level,\n",
    "                                                                             self.config.aspect_embed_type))\n",
    "            if config.aspect_embed_type == 'random':\n",
    "                self.n_aspect = self.aspect_embeddings.shape[0]\n",
    "                self.aspect_embeddings = None\n",
    "        else:\n",
    "            self.aspect_embeddings = None\n",
    "        if self.config.use_aspect_text_input:\n",
    "            self.aspect_text_embeddings = np.load('./data/%s/aspect_text_%s_%s.npy' % (self.config.data_dir,\n",
    "                                                                                       self.level,\n",
    "                                                                                       self.config.word_embed_type))\n",
    "            self.config.idx2aspect_token = load_token(self.config.data_dir, 'aspect_text_{}'.format(self.level))\n",
    "        else:\n",
    "            self.aspect_text_embeddings = None\n",
    "\n",
    "        self.callbacks = []\n",
    "        self.init_callbacks()\n",
    "\n",
    "        self.model = None\n",
    "        self.build_model()\n",
    "\n",
    "    def init_callbacks(self):\n",
    "        self.callbacks.append(SentimentModelMetrics())\n",
    "\n",
    "        self.callbacks.append(ModelCheckpoint(\n",
    "            filepath=os.path.join(self.config.checkpoint_dir, '%s/%s.hdf5' % (self.config.data_dir,\n",
    "                                                                              self.config.exp_name)),\n",
    "            monitor=self.config.checkpoint_monitor,\n",
    "            save_best_only=self.config.checkpoint_save_best_only,\n",
    "            save_weights_only=self.config.checkpoint_save_weights_only,\n",
    "            mode=self.config.checkpoint_save_weights_mode,\n",
    "            verbose=self.config.checkpoint_verbose\n",
    "        ))\n",
    "\n",
    "        # self.callbacks.append(EarlyStopping(\n",
    "        #     monitor=self.config.early_stopping_monitor,\n",
    "        #     mode=self.config.early_stopping_mode,\n",
    "        #     patience=self.config.early_stopping_patience\n",
    "        # ))\n",
    "\n",
    "    def load(self):\n",
    "        print('loading model checkpoint {} ...\\n'.format('%s.hdf5') % self.config.exp_name)\n",
    "        self.model.load_weights(os.path.join(self.config.checkpoint_dir, '%s/%s.hdf5' % (self.config.data_dir,\n",
    "                                                                                         self.config.exp_name)))\n",
    "        print('Model loaded')\n",
    "\n",
    "    def build_base_network(self):\n",
    "        if self.config.modelName == 'td_lstm':\n",
    "            base_network = self.td_lstm()\n",
    "        elif self.config.modelName == 'tc_lstm':\n",
    "            base_network = self.tc_lstm()\n",
    "        elif self.config.modelName == 'atae_lstm':\n",
    "            base_network = self.atae_lstm()\n",
    "        elif self.config.modelName == 'memnet':\n",
    "            base_network = self.memnet()\n",
    "        else:\n",
    "            raise Exception('Model Name `%s` Not Understood' % self.config.modelName)\n",
    "\n",
    "        return base_network\n",
    "\n",
    "    def build_model(self):\n",
    "        network_inputs = list()\n",
    "        if self.config.use_text_input:\n",
    "            network_inputs.append(Input(shape=(self.max_len,), name='input_text'))\n",
    "        if self.config.use_text_input_l:\n",
    "             network_inputs.append(Input(shape=(self.left_max_len,), name='input_text_l'))\n",
    "        if self.config.use_text_input_r:\n",
    "            network_inputs.append(Input(shape=(self.right_max_len,), name='input_text_r'))\n",
    "        if self.config.use_text_input_r_with_pad:\n",
    "            network_inputs.append(Input(shape=(self.max_len,), name='input_text_r_with_pad'))\n",
    "        if self.config.use_aspect_input:\n",
    "            network_inputs.append(Input(shape=(1, ), name='input_aspect'))\n",
    "        if self.config.use_aspect_text_input:\n",
    "            network_inputs.append(Input(shape=(self.asp_max_len,), name='input_aspect_text'))\n",
    "        if self.config.use_loc_input:\n",
    "            network_inputs.append(Input(shape=(self.max_len,), name='input_loc_info'))\n",
    "        if self.config.use_offset_input:\n",
    "            network_inputs.append(Input(shape=(self.max_len,), name='input_offset_info'))\n",
    "        if self.config.use_mask:\n",
    "            network_inputs.append(Input(shape=(self.max_len,), name='input_mask'))\n",
    "\n",
    "        if len(network_inputs) == 1:\n",
    "            network_inputs = network_inputs[0]\n",
    "        elif len(network_inputs) == 0:\n",
    "            raise Exception('No Input!')\n",
    "\n",
    "        base_network = self.build_base_network()\n",
    "        sentence_vec = base_network(network_inputs)\n",
    "        dense_layer = Dense(self.config.dense_units, activation='relu',kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1))(sentence_vec)\n",
    "        #dense_layer = Dense(self.config.n_classes, activation='softmax')(sentence_vec)\n",
    "        output_layer = Dense(self.config.n_classes, activation='softmax',kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1))(dense_layer)\n",
    "\n",
    "        #dense_layer = Dense(self.config.dense_units, activation='relu')(sentence_vec)\n",
    "        \n",
    "        #output_layer = Dense(self.config.n_classes, activation='softmax')(dense_layer)\n",
    "        self.model = Model(network_inputs, output_layer)\n",
    "        self.model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=self.config.optimizer)\n",
    "\n",
    "    def prepare_input(self, input_data):\n",
    "        if self.config.modelName == 'td_lstm':\n",
    "            text_l, text_r = input_data\n",
    "            input_pad = [pad_sequences(text_l, self.left_max_len), pad_sequences(text_r, self.right_max_len)]\n",
    "        elif self.config.modelName == 'tc_lstm':\n",
    "            text_l, text_r, aspect = input_data\n",
    "            input_pad = [pad_sequences(text_l, self.left_max_len), pad_sequences(text_r, self.right_max_len),\n",
    "                         np.array(aspect)]\n",
    "        elif self.config.modelName in ['atae_lstm'] or \\\n",
    "                (self.config.modelName in ['memnet'] and not self.config.is_aspect_term):\n",
    "            text, aspect = input_data\n",
    "            input_pad = [pad_sequences(text, self.max_len), np.array(aspect)]\n",
    "        elif self.config.modelName == 'memnet' and self.config.is_aspect_term:\n",
    "            text, aspect, loc = input_data\n",
    "            input_pad = [pad_sequences(text, self.max_len), np.array(aspect), pad_sequences(loc, self.max_len)]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('model name `{}` not understood'.format(self.config.modelName))\n",
    "        return input_pad\n",
    "\n",
    "    def prepare_label(self, label_data):\n",
    "        return to_categorical(label_data, self.config.n_classes)\n",
    "    \n",
    "\n",
    "    def train(self, train_input_data, train_label, valid_input_data, valid_label):\n",
    "        x_train = self.prepare_input(train_input_data)\n",
    "        y_train = self.prepare_label(train_label)\n",
    "        x_valid = self.prepare_input(valid_input_data)\n",
    "        y_valid = self.prepare_label(valid_label)\n",
    "\n",
    "        print('start training...')\n",
    "        history = self.model.fit(x=x_train, y=y_train, batch_size=self.config.batch_size, epochs=self.config.n_epochs,\n",
    "                       validation_data=(x_valid, y_valid), callbacks=self.callbacks)\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('training end...')\n",
    "        self.model.save('%s/%s.h5' % (self.config.data_dir,self.config.exp_name))\n",
    "        print('score over valid data:')\n",
    "        valid_pred = self.model.predict(x_valid)\n",
    "        #print(\"Valid pred\")\n",
    "        #print(valid_pred)\n",
    "        get_sentiment_score(y_valid, valid_pred)\n",
    "\n",
    "    def score(self, input_data, label):\n",
    "        input_pad = self.prepare_input(input_data)\n",
    "        label = self.prepare_label(label)\n",
    "        prediction = self.model.predict(input_pad)\n",
    "        get_sentiment_score(label, prediction)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        input_pad = self.prepare_input(input_data)\n",
    "        prediction = self.model.predict(input_pad)\n",
    "        return np.argmax(prediction, axis=-1)\n",
    "\n",
    "    # target dependent lstm\n",
    "    def td_lstm(self):\n",
    "        input_l = Input(shape=(self.left_max_len, ))\n",
    "        input_r = Input(shape=(self.right_max_len, ))\n",
    "\n",
    "        word_embedding = Embedding(input_dim=self.text_embeddings.shape[0], output_dim=self.config.word_embed_dim,\n",
    "                                       weights=[self.text_embeddings], trainable=self.config.word_embed_trainable,\n",
    "                                       mask_zero=True)\n",
    "        input_l_embed = SpatialDropout1D(0.2)(word_embedding(input_l))\n",
    "        input_r_embed = SpatialDropout1D(0.2)(word_embedding(input_r))\n",
    "\n",
    "        # regarding aspect string as the last unit\n",
    "        hidden_l = LSTM(self.config.lstm_units)(input_l_embed)\n",
    "        hidden_r = LSTM(self.config.lstm_units, go_backwards=True)(input_r_embed)\n",
    "\n",
    "        hidden_concat = concatenate([hidden_l, hidden_r], axis=-1)\n",
    "\n",
    "        return Model([input_l, input_r], hidden_concat)\n",
    "\n",
    "    # target connection lstm\n",
    "    def tc_lstm(self):\n",
    "        input_l = Input(shape=(self.left_max_len,))\n",
    "        input_r = Input(shape=(self.right_max_len,))\n",
    "        input_aspect = Input(shape=(1,))\n",
    "\n",
    "        word_embedding = Embedding(input_dim=self.text_embeddings.shape[0], output_dim=self.config.word_embed_dim,\n",
    "                                       weights=[self.text_embeddings], trainable=self.config.word_embed_trainable,\n",
    "                                       mask_zero=True)\n",
    "        input_l_embed = SpatialDropout1D(0.2)(word_embedding(input_l))\n",
    "        input_r_embed = SpatialDropout1D(0.2)(word_embedding(input_r))\n",
    "\n",
    "        if self.config.aspect_embed_type == 'random':\n",
    "            asp_embedding = Embedding(input_dim=self.n_aspect, output_dim=self.config.aspect_embed_dim)\n",
    "        else:\n",
    "            asp_embedding = Embedding(input_dim=self.aspect_embeddings.shape[0],\n",
    "                                      output_dim=self.config.aspect_embed_dim,\n",
    "                                      trainable=self.config.aspect_embed_trainable)\n",
    "        aspect_embed = asp_embedding(input_aspect)\n",
    "        aspect_embed = Flatten()(aspect_embed)\n",
    "\n",
    "        aspect_repeat_l = RepeatVector(self.left_max_len)(aspect_embed)\n",
    "        input_l_concat = concatenate([input_l_embed, aspect_repeat_l], axis=-1)\n",
    "        aspect_repeat_r = RepeatVector(self.right_max_len)(aspect_embed)\n",
    "        input_r_concat = concatenate([input_r_embed, aspect_repeat_r], axis=-1)\n",
    "\n",
    "        # regarding aspect string as the last unit\n",
    "        hidden_l = LSTM(self.config.lstm_units)(input_l_concat)\n",
    "        hidden_r = LSTM(self.config.lstm_units, go_backwards=True)(input_r_concat)\n",
    "\n",
    "        hidden_concat = concatenate([hidden_l, hidden_r], axis=-1)\n",
    "\n",
    "        return Model([input_l, input_r, input_aspect], hidden_concat)\n",
    "\n",
    "\n",
    "    def atae_lstm(self):\n",
    "        input_text = Input(shape=(self.max_len,))\n",
    "        input_aspect = Input(shape=(1,), )\n",
    "\n",
    "        word_embedding = Embedding(input_dim=self.text_embeddings.shape[0], output_dim=self.config.word_embed_dim,\n",
    "                                       weights=[self.text_embeddings], trainable=self.config.word_embed_trainable,\n",
    "                                       mask_zero=True)\n",
    "        text_embed = SpatialDropout1D(0.2)(word_embedding(input_text))\n",
    "\n",
    "        if self.config.aspect_embed_type == 'random':\n",
    "            asp_embedding = Embedding(input_dim=self.n_aspect, output_dim=self.config.aspect_embed_dim)\n",
    "        else:\n",
    "            asp_embedding = Embedding(input_dim=self.aspect_embeddings.shape[0],\n",
    "                                      output_dim=self.config.aspect_embed_dim,\n",
    "                                      trainable=self.config.aspect_embed_trainable)\n",
    "        aspect_embed = asp_embedding(input_aspect)\n",
    "        aspect_embed = Flatten()(aspect_embed)  # reshape to 2d\n",
    "        repeat_aspect = RepeatVector(self.max_len)(aspect_embed)  # repeat aspect for every word in sequence\n",
    "\n",
    "        input_concat = concatenate([text_embed, repeat_aspect], axis=-1)\n",
    "        hidden_vecs, state_h, _ = LSTM(self.config.lstm_units, return_sequences=True, return_state=True)(input_concat)\n",
    "        concat = concatenate([hidden_vecs, repeat_aspect], axis=-1)\n",
    "\n",
    "        # apply attention mechanism\n",
    "        attend_weight = Attention()(concat)\n",
    "        attend_weight_expand = Lambda(lambda x: K.expand_dims(x))(attend_weight)\n",
    "        attend_hidden = multiply([hidden_vecs, attend_weight_expand])\n",
    "        attend_hidden = Lambda(lambda x: K.sum(x, axis=1))(attend_hidden)\n",
    "\n",
    "        attend_hidden_dense = Dense(self.config.lstm_units)(attend_hidden)\n",
    "        last_hidden_dense = Dense(self.config.lstm_units)(state_h)\n",
    "        final_output = Activation('tanh')(add([attend_hidden_dense, last_hidden_dense]))\n",
    "\n",
    "        return Model([input_text, input_aspect], final_output)\n",
    "\n",
    "    # deep memory network\n",
    "    def memnet(self):\n",
    "        n_hop = 9\n",
    "        input_text = Input(shape=(self.max_len,))\n",
    "        input_aspect = Input(shape=(1,))\n",
    "        inputs = [input_text, input_aspect]\n",
    "\n",
    "        word_embedding = Embedding(input_dim=self.text_embeddings.shape[0], output_dim=self.config.word_embed_dim,\n",
    "                                   weights=[self.text_embeddings], trainable=self.config.word_embed_trainable,\n",
    "                                   mask_zero=True)\n",
    "        text_embed = SpatialDropout1D(0.2)(word_embedding(input_text))\n",
    "\n",
    "        if self.config.use_loc_input:   # location attention\n",
    "            input_loc = Input(shape=(self.max_len,))\n",
    "            inputs.append(input_loc)\n",
    "            input_loc_expand = Lambda(lambda x: K.expand_dims(x))(input_loc)\n",
    "            text_embed = multiply([text_embed, input_loc_expand])\n",
    "\n",
    "        if self.config.aspect_embed_type == 'random':\n",
    "            asp_embedding = Embedding(input_dim=self.n_aspect, output_dim=self.config.aspect_embed_dim)\n",
    "        else:\n",
    "            asp_embedding = Embedding(input_dim=self.aspect_embeddings.shape[0],\n",
    "                                      output_dim=self.config.aspect_embed_dim,\n",
    "                                      trainable=self.config.aspect_embed_trainable)\n",
    "        aspect_embed = asp_embedding(input_aspect)\n",
    "        #print(\"Input aspect\")\n",
    "        #print(input_aspect)\n",
    "        aspect_embed = Flatten()(aspect_embed)  # reshape to 2d\n",
    "        #print(aspect_embed)\n",
    "        # the parameter of attention and linear layers are shared in different hops\n",
    "        attention_layer = Attention(use_W=False, use_bias=True)\n",
    "        linear_layer = Dense(self.config.word_embed_dim)\n",
    "        # output from each computation layer, representing text in different level of abstraction\n",
    "        computation_layers_out = [aspect_embed]\n",
    "\n",
    "        for h in range(n_hop):\n",
    "            # content attention layer\n",
    "            repeat_out = RepeatVector(self.max_len)(computation_layers_out[-1])\n",
    "            concat = concatenate([text_embed, repeat_out], axis=-1)\n",
    "            attend_weight = attention_layer(concat)\n",
    "            attend_weight_expand = Lambda(lambda x: K.expand_dims(x))(attend_weight)\n",
    "            content_attend = multiply([text_embed, attend_weight_expand])\n",
    "            content_attend = Lambda(lambda x: K.sum(x, axis=1))(content_attend)\n",
    "\n",
    "            # linear layer\n",
    "            out_linear = linear_layer(computation_layers_out[-1])\n",
    "            computation_layers_out.append(add([content_attend, out_linear]))\n",
    "        return Model(inputs, computation_layers_out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
